{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Important libaries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-05T06:26:37.129995Z","iopub.execute_input":"2023-10-05T06:26:37.130369Z","iopub.status.idle":"2023-10-05T06:26:37.135063Z","shell.execute_reply.started":"2023-10-05T06:26:37.130342Z","shell.execute_reply":"2023-10-05T06:26:37.134254Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# plot Accurcy graph for model tuning?\nplot_knn_accuracy = False\nplot_logreg_accuracy = False","metadata":{"execution":{"iopub.status.busy":"2023-10-05T06:26:37.154575Z","iopub.execute_input":"2023-10-05T06:26:37.154976Z","iopub.status.idle":"2023-10-05T06:26:37.160351Z","shell.execute_reply.started":"2023-10-05T06:26:37.154924Z","shell.execute_reply":"2023-10-05T06:26:37.159264Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"# Read TSV data\ncolumns = [\"ID\", \"label_6\", \"statement\", \"subject\", \"speaker\", \"speaker_job\", \"state\", \"party\", \"pof_count\", \"false_count\", \"barelytrue_count\", \"halftrue_count\", \"mostlytrue_count\", \"context\"]\ntrain_data = pd.read_table(\"/kaggle/input/LIARbyYang/train.tsv\", header = None, names = columns)\nv_data = pd.read_table(\"/kaggle/input/LIARbyYang/valid.tsv\", header = None, names = columns)\ntest_data = pd.read_table(\"/kaggle/input/LIARbyYang/test.tsv\", header = None, names = columns)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-05T06:26:37.179771Z","iopub.execute_input":"2023-10-05T06:26:37.180192Z","iopub.status.idle":"2023-10-05T06:26:37.264509Z","shell.execute_reply.started":"2023-10-05T06:26:37.180160Z","shell.execute_reply":"2023-10-05T06:26:37.263398Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"Here we do not want to merge the three tables provided. Next, check the data:","metadata":{}},{"cell_type":"code","source":"train_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T06:26:37.266246Z","iopub.execute_input":"2023-10-05T06:26:37.266586Z","iopub.status.idle":"2023-10-05T06:26:37.289135Z","shell.execute_reply.started":"2023-10-05T06:26:37.266556Z","shell.execute_reply":"2023-10-05T06:26:37.288328Z"},"trusted":true},"execution_count":97,"outputs":[{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"           ID      label_6                                          statement  \\\n0   2635.json        false  Says the Annies List political group supports ...   \n1  10540.json    half-true  When did the decline of coal start? It started...   \n2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n3   1123.json        false  Health care reform legislation is likely to ma...   \n4   9028.json    half-true  The economic turnaround started at the end of ...   \n5  12465.json         true  The Chicago Bears have had more starting quart...   \n6   2342.json  barely-true  Jim Dunnam has not lived in the district he re...   \n7    153.json    half-true  I'm the only person on this stage who has work...   \n8   5602.json    half-true  However, it took $19.5 million in Oregon Lotte...   \n9   9741.json  mostly-true  Says GOP primary opponents Glenn Grothman and ...   \n\n                                     subject                 speaker  \\\n0                                   abortion            dwayne-bohac   \n1         energy,history,job-accomplishments          scott-surovell   \n2                             foreign-policy            barack-obama   \n3                                health-care            blog-posting   \n4                               economy,jobs           charlie-crist   \n5                                  education               robin-vos   \n6                       candidates-biography  republican-party-texas   \n7                                     ethics            barack-obama   \n8                                       jobs          oregon-lottery   \n9  energy,message-machine-2014,voting-record           duey-stroebel   \n\n                  speaker_job      state         party  pof_count  \\\n0        State representative      Texas    republican        0.0   \n1              State delegate   Virginia      democrat        0.0   \n2                   President   Illinois      democrat       70.0   \n3                         NaN        NaN          none        7.0   \n4                         NaN    Florida      democrat       15.0   \n5  Wisconsin Assembly speaker  Wisconsin    republican        0.0   \n6                         NaN      Texas    republican        3.0   \n7                   President   Illinois      democrat       70.0   \n8                         NaN        NaN  organization        0.0   \n9        State representative  Wisconsin    republican        0.0   \n\n   false_count  barelytrue_count  halftrue_count  mostlytrue_count  \\\n0          1.0               0.0             0.0               0.0   \n1          0.0               1.0             1.0               0.0   \n2         71.0             160.0           163.0               9.0   \n3         19.0               3.0             5.0              44.0   \n4          9.0              20.0            19.0               2.0   \n5          3.0               2.0             5.0               1.0   \n6          1.0               1.0             3.0               1.0   \n7         71.0             160.0           163.0               9.0   \n8          0.0               1.0             0.0               1.0   \n9          0.0               0.0             1.0               0.0   \n\n                                    context  \n0                                  a mailer  \n1                           a floor speech.  \n2                                    Denver  \n3                            a news release  \n4                       an interview on CNN  \n5                 a an online opinion-piece  \n6                          a press release.  \n7  a Democratic debate in Philadelphia, Pa.  \n8                                a website   \n9                           an online video  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>label_6</th>\n      <th>statement</th>\n      <th>subject</th>\n      <th>speaker</th>\n      <th>speaker_job</th>\n      <th>state</th>\n      <th>party</th>\n      <th>pof_count</th>\n      <th>false_count</th>\n      <th>barelytrue_count</th>\n      <th>halftrue_count</th>\n      <th>mostlytrue_count</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2635.json</td>\n      <td>false</td>\n      <td>Says the Annies List political group supports ...</td>\n      <td>abortion</td>\n      <td>dwayne-bohac</td>\n      <td>State representative</td>\n      <td>Texas</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>a mailer</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10540.json</td>\n      <td>half-true</td>\n      <td>When did the decline of coal start? It started...</td>\n      <td>energy,history,job-accomplishments</td>\n      <td>scott-surovell</td>\n      <td>State delegate</td>\n      <td>Virginia</td>\n      <td>democrat</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>a floor speech.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>324.json</td>\n      <td>mostly-true</td>\n      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n      <td>foreign-policy</td>\n      <td>barack-obama</td>\n      <td>President</td>\n      <td>Illinois</td>\n      <td>democrat</td>\n      <td>70.0</td>\n      <td>71.0</td>\n      <td>160.0</td>\n      <td>163.0</td>\n      <td>9.0</td>\n      <td>Denver</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1123.json</td>\n      <td>false</td>\n      <td>Health care reform legislation is likely to ma...</td>\n      <td>health-care</td>\n      <td>blog-posting</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>none</td>\n      <td>7.0</td>\n      <td>19.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>44.0</td>\n      <td>a news release</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9028.json</td>\n      <td>half-true</td>\n      <td>The economic turnaround started at the end of ...</td>\n      <td>economy,jobs</td>\n      <td>charlie-crist</td>\n      <td>NaN</td>\n      <td>Florida</td>\n      <td>democrat</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>20.0</td>\n      <td>19.0</td>\n      <td>2.0</td>\n      <td>an interview on CNN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12465.json</td>\n      <td>true</td>\n      <td>The Chicago Bears have had more starting quart...</td>\n      <td>education</td>\n      <td>robin-vos</td>\n      <td>Wisconsin Assembly speaker</td>\n      <td>Wisconsin</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>a an online opinion-piece</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2342.json</td>\n      <td>barely-true</td>\n      <td>Jim Dunnam has not lived in the district he re...</td>\n      <td>candidates-biography</td>\n      <td>republican-party-texas</td>\n      <td>NaN</td>\n      <td>Texas</td>\n      <td>republican</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>a press release.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>153.json</td>\n      <td>half-true</td>\n      <td>I'm the only person on this stage who has work...</td>\n      <td>ethics</td>\n      <td>barack-obama</td>\n      <td>President</td>\n      <td>Illinois</td>\n      <td>democrat</td>\n      <td>70.0</td>\n      <td>71.0</td>\n      <td>160.0</td>\n      <td>163.0</td>\n      <td>9.0</td>\n      <td>a Democratic debate in Philadelphia, Pa.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>5602.json</td>\n      <td>half-true</td>\n      <td>However, it took $19.5 million in Oregon Lotte...</td>\n      <td>jobs</td>\n      <td>oregon-lottery</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>organization</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>a website</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9741.json</td>\n      <td>mostly-true</td>\n      <td>Says GOP primary opponents Glenn Grothman and ...</td>\n      <td>energy,message-machine-2014,voting-record</td>\n      <td>duey-stroebel</td>\n      <td>State representative</td>\n      <td>Wisconsin</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>an online video</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-05T06:26:37.290180Z","iopub.execute_input":"2023-10-05T06:26:37.291137Z","iopub.status.idle":"2023-10-05T06:26:37.297273Z","shell.execute_reply.started":"2023-10-05T06:26:37.291105Z","shell.execute_reply":"2023-10-05T06:26:37.296205Z"},"trusted":true},"execution_count":98,"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"(10240, 14)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Relabel","metadata":{}},{"cell_type":"markdown","source":"We want to merge the labels from 6-ways to 2 and 3-ways, while keeping the original 6-ways labels for the benchmark.","metadata":{}},{"cell_type":"code","source":"merger_2 = { 'pants-fire' : 0,\n           'false' : 0,\n           'barely-true': 0,\n           'half-true': 0,\n           'mostly-true': 1,\n           'true': 1}\nmerger_3 = { 'pants-fire' : 0,\n           'false' : 0,\n           'barely-true': 1,\n           'half-true': 1,\n           'mostly-true': 2,\n           'true': 2}\nmerger_6 = { 'pants-fire' : 0,\n           'false' : 1,\n           'barely-true': 2,\n           'half-true': 3,\n           'mostly-true': 4,\n           'true': 5}\n\n# training set\ntrain_data['label_2'] = train_data[\"label_6\"].map(merger_2)\ntrain_data['label_3'] = train_data[\"label_6\"].map(merger_3)\ntrain_data[\"label_6\"].replace(merger_6, inplace = True)\n\n# cross validation set\nv_data['label_2'] = v_data[\"label_6\"].map(merger_2)\nv_data['label_3'] = v_data[\"label_6\"].map(merger_3)\nv_data[\"label_6\"].replace(merger_6, inplace = True)\n\n# test set\ntest_data['label_2'] = test_data[\"label_6\"].map(merger_2)\ntest_data['label_3'] = test_data[\"label_6\"].map(merger_3)\ntest_data[\"label_6\"].replace(merger_6, inplace = True)\n\n# check\ntrain_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T06:26:37.309701Z","iopub.execute_input":"2023-10-05T06:26:37.311518Z","iopub.status.idle":"2023-10-05T06:26:37.357164Z","shell.execute_reply.started":"2023-10-05T06:26:37.311477Z","shell.execute_reply":"2023-10-05T06:26:37.356044Z"},"trusted":true},"execution_count":99,"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"           ID  label_6                                          statement  \\\n0   2635.json        1  Says the Annies List political group supports ...   \n1  10540.json        3  When did the decline of coal start? It started...   \n2    324.json        4  Hillary Clinton agrees with John McCain \"by vo...   \n3   1123.json        1  Health care reform legislation is likely to ma...   \n4   9028.json        3  The economic turnaround started at the end of ...   \n5  12465.json        5  The Chicago Bears have had more starting quart...   \n6   2342.json        2  Jim Dunnam has not lived in the district he re...   \n7    153.json        3  I'm the only person on this stage who has work...   \n8   5602.json        3  However, it took $19.5 million in Oregon Lotte...   \n9   9741.json        4  Says GOP primary opponents Glenn Grothman and ...   \n\n                                     subject                 speaker  \\\n0                                   abortion            dwayne-bohac   \n1         energy,history,job-accomplishments          scott-surovell   \n2                             foreign-policy            barack-obama   \n3                                health-care            blog-posting   \n4                               economy,jobs           charlie-crist   \n5                                  education               robin-vos   \n6                       candidates-biography  republican-party-texas   \n7                                     ethics            barack-obama   \n8                                       jobs          oregon-lottery   \n9  energy,message-machine-2014,voting-record           duey-stroebel   \n\n                  speaker_job      state         party  pof_count  \\\n0        State representative      Texas    republican        0.0   \n1              State delegate   Virginia      democrat        0.0   \n2                   President   Illinois      democrat       70.0   \n3                         NaN        NaN          none        7.0   \n4                         NaN    Florida      democrat       15.0   \n5  Wisconsin Assembly speaker  Wisconsin    republican        0.0   \n6                         NaN      Texas    republican        3.0   \n7                   President   Illinois      democrat       70.0   \n8                         NaN        NaN  organization        0.0   \n9        State representative  Wisconsin    republican        0.0   \n\n   false_count  barelytrue_count  halftrue_count  mostlytrue_count  \\\n0          1.0               0.0             0.0               0.0   \n1          0.0               1.0             1.0               0.0   \n2         71.0             160.0           163.0               9.0   \n3         19.0               3.0             5.0              44.0   \n4          9.0              20.0            19.0               2.0   \n5          3.0               2.0             5.0               1.0   \n6          1.0               1.0             3.0               1.0   \n7         71.0             160.0           163.0               9.0   \n8          0.0               1.0             0.0               1.0   \n9          0.0               0.0             1.0               0.0   \n\n                                    context  label_2  label_3  \n0                                  a mailer        0        0  \n1                           a floor speech.        0        1  \n2                                    Denver        1        2  \n3                            a news release        0        0  \n4                       an interview on CNN        0        1  \n5                 a an online opinion-piece        1        2  \n6                          a press release.        0        1  \n7  a Democratic debate in Philadelphia, Pa.        0        1  \n8                                a website         0        1  \n9                           an online video        1        2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>label_6</th>\n      <th>statement</th>\n      <th>subject</th>\n      <th>speaker</th>\n      <th>speaker_job</th>\n      <th>state</th>\n      <th>party</th>\n      <th>pof_count</th>\n      <th>false_count</th>\n      <th>barelytrue_count</th>\n      <th>halftrue_count</th>\n      <th>mostlytrue_count</th>\n      <th>context</th>\n      <th>label_2</th>\n      <th>label_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2635.json</td>\n      <td>1</td>\n      <td>Says the Annies List political group supports ...</td>\n      <td>abortion</td>\n      <td>dwayne-bohac</td>\n      <td>State representative</td>\n      <td>Texas</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>a mailer</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10540.json</td>\n      <td>3</td>\n      <td>When did the decline of coal start? It started...</td>\n      <td>energy,history,job-accomplishments</td>\n      <td>scott-surovell</td>\n      <td>State delegate</td>\n      <td>Virginia</td>\n      <td>democrat</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>a floor speech.</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>324.json</td>\n      <td>4</td>\n      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n      <td>foreign-policy</td>\n      <td>barack-obama</td>\n      <td>President</td>\n      <td>Illinois</td>\n      <td>democrat</td>\n      <td>70.0</td>\n      <td>71.0</td>\n      <td>160.0</td>\n      <td>163.0</td>\n      <td>9.0</td>\n      <td>Denver</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1123.json</td>\n      <td>1</td>\n      <td>Health care reform legislation is likely to ma...</td>\n      <td>health-care</td>\n      <td>blog-posting</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>none</td>\n      <td>7.0</td>\n      <td>19.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>44.0</td>\n      <td>a news release</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9028.json</td>\n      <td>3</td>\n      <td>The economic turnaround started at the end of ...</td>\n      <td>economy,jobs</td>\n      <td>charlie-crist</td>\n      <td>NaN</td>\n      <td>Florida</td>\n      <td>democrat</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>20.0</td>\n      <td>19.0</td>\n      <td>2.0</td>\n      <td>an interview on CNN</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12465.json</td>\n      <td>5</td>\n      <td>The Chicago Bears have had more starting quart...</td>\n      <td>education</td>\n      <td>robin-vos</td>\n      <td>Wisconsin Assembly speaker</td>\n      <td>Wisconsin</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>a an online opinion-piece</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2342.json</td>\n      <td>2</td>\n      <td>Jim Dunnam has not lived in the district he re...</td>\n      <td>candidates-biography</td>\n      <td>republican-party-texas</td>\n      <td>NaN</td>\n      <td>Texas</td>\n      <td>republican</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>a press release.</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>153.json</td>\n      <td>3</td>\n      <td>I'm the only person on this stage who has work...</td>\n      <td>ethics</td>\n      <td>barack-obama</td>\n      <td>President</td>\n      <td>Illinois</td>\n      <td>democrat</td>\n      <td>70.0</td>\n      <td>71.0</td>\n      <td>160.0</td>\n      <td>163.0</td>\n      <td>9.0</td>\n      <td>a Democratic debate in Philadelphia, Pa.</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>5602.json</td>\n      <td>3</td>\n      <td>However, it took $19.5 million in Oregon Lotte...</td>\n      <td>jobs</td>\n      <td>oregon-lottery</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>organization</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>a website</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9741.json</td>\n      <td>4</td>\n      <td>Says GOP primary opponents Glenn Grothman and ...</td>\n      <td>energy,message-machine-2014,voting-record</td>\n      <td>duey-stroebel</td>\n      <td>State representative</td>\n      <td>Wisconsin</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>an online video</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Bag of Words Process","metadata":{}},{"cell_type":"code","source":"# tokenization processes\n\n\nfrom nltk.corpus import stopwords\nimport string\nfrom nltk.tokenize import WhitespaceTokenizer as w_tokenizer\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer=SnowballStemmer(\"english\")\nstop_words = set(stopwords.words('english'))\n\n# remove stopwords\ndef remove_stopwords(list_of_tokens):\n    new_list_of_tokens = [w for w in list_of_tokens if not w.lower() in stop_words]\n    return new_list_of_tokens\n\n# remove punctutations and tokens with less than two in length\ndef remove_punctuations_shortwords(list_of_tokens):\n    translation = str.maketrans('', '', string.punctuation)\n    new_list_of_tokens = [tokens.translate(translation) for tokens in list_of_tokens if len(tokens.translate(translation))>=3]\n    return new_list_of_tokens\n\ndef tokenize_and_stem(Pandas_Series):\n    Pandas_Series = Pandas_Series.replace('\\d+', '', regex=True) # remove numbers\n    Pandas_Series = Pandas_Series.apply(nltk.word_tokenize) # tokenize statement\n    Pandas_Series = Pandas_Series.apply(remove_stopwords) # remove stopwords\n    Pandas_Series = Pandas_Series.apply(remove_punctuations_shortwords) # remove punctutations and tokens with less than two in length\n    Pandas_Series = Pandas_Series.apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n    return Pandas_Series","metadata":{"execution":{"iopub.status.busy":"2023-10-05T06:26:37.358874Z","iopub.execute_input":"2023-10-05T06:26:37.359240Z","iopub.status.idle":"2023-10-05T06:26:37.367643Z","shell.execute_reply.started":"2023-10-05T06:26:37.359210Z","shell.execute_reply":"2023-10-05T06:26:37.366896Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"train_data['stemmed_statement_token'] = tokenize_and_stem(train_data['statement']) # Process statement strings","metadata":{"execution":{"iopub.status.busy":"2023-10-05T06:26:37.374353Z","iopub.execute_input":"2023-10-05T06:26:37.375160Z","iopub.status.idle":"2023-10-05T06:26:40.849739Z","shell.execute_reply.started":"2023-10-05T06:26:37.375124Z","shell.execute_reply":"2023-10-05T06:26:40.848369Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"See sentence length distribution:","metadata":{}},{"cell_type":"code","source":"sns.histplot(train_data['stemmed_statement_token'].str.len())","metadata":{"execution":{"iopub.status.busy":"2023-10-05T06:26:40.851560Z","iopub.execute_input":"2023-10-05T06:26:40.852034Z"},"trusted":true},"execution_count":null,"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"<Axes: xlabel='stemmed_statement_token', ylabel='Count'>"},"metadata":{}}]},{"cell_type":"markdown","source":"One can see that there are some outlier sentences that exceed 30 tokens. Our approach is not to get rid of their records, but to exclude them from the CountVecterizer fitting. To do that, we keep the index of instances with statements having too much tokens.","metadata":{}},{"cell_type":"code","source":"statement_index_tofit = train_data['stemmed_statement_token'].str.len() < 30\ntrain_data[-statement_index_tofit] # instances with very long statements","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See the most common (and potentially useless) words for the dataset, check for redundant words:","metadata":{}},{"cell_type":"code","source":"# bag of words for the whole dataset\ndef countwords(x):\n    word2count = {}\n    for words in x:\n        for word in words:\n            if word not in word2count.keys():\n                word2count[word] = 1\n            else:\n                word2count[word] += 1\n    # sort the word2count\n    sorted_word2count = sorted(word2count.items(), key=lambda x:x[1], reverse = True)\n    return sorted_word2count\n            \nwords_in_statements = countwords(train_data[\"stemmed_statement_token\"])\nwords_in_statements[0:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Consider the n-grams","metadata":{}},{"cell_type":"code","source":"# n-grams\ndef build_bigrams(token):\n    if len(token) >= 1:\n        x = list(nltk.ngrams(token, 2))\n    else:\n        x = []\n    return x\n\ndef build_trigrams(token):\n    if len(token) >= 2:\n        x = list(nltk.ngrams(token, 3))\n    else:\n        x = []\n    return x\n\ntrain_data[\"stemmed_statement_bigrams\"]=train_data[\"stemmed_statement_token\"].apply(build_bigrams)\ntrain_data[\"stemmed_statement_trigrams\"]=train_data[\"stemmed_statement_token\"].apply(build_trigrams)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply CountVectorizer, ignore the first few rendundant words and words that appear only once.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nv_statement = CountVectorizer(analyzer=lambda x: x, max_df = 1.0 ,min_df = 2) # need analyzer=lambda x: x to disable the analyzer, since the input is pandas series of lists.\nv_statement.fit(train_data['stemmed_statement_bigrams'][statement_index_tofit])\nvectors = v_statement.transform(train_data['stemmed_statement_bigrams'])\n\ndef new_features(X, string, vectorizer):\n    new_features = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names_out()) # Get new features from countvectorizer\n    if type(new_features.columns[0]) == tuple:\n        new_features.rename(columns='_'.join, inplace=True)\n    new_features.columns = string + '_' + new_features.columns # add prefix to the column names\n    return new_features\n\nstatement_vector = new_features(vectors, 'st', v_statement)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"statement_vector.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(np.sum(statement_vector, axis = 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Merge new features","metadata":{}},{"cell_type":"code","source":"train_data2 = pd.concat([train_data,statement_vector],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do the same for context. But there are 102 null present, drop them first.","metadata":{}},{"cell_type":"code","source":"sum(train_data2['context'].isnull())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Suppose null value for the context is caused by unknown source, replace them with \"unknown\".","metadata":{}},{"cell_type":"code","source":"# fill null context\ntrain_data2[\"context\"].fillna(value = \"unknown\", inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data2['stemmed_context_token'] = tokenize_and_stem(train_data2['context']) # Process context strings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check redundant:","metadata":{}},{"cell_type":"code","source":"words_in_context = countwords(train_data2[\"stemmed_context_token\"])\nprint(words_in_context[0:20])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(words_in_context)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply vectorizer:","metadata":{}},{"cell_type":"code","source":"v_context = CountVectorizer(analyzer=lambda x: x, max_df = 1800 ,min_df = 50) # need analyzer=lambda x: x to disable the analyzer, since the input is pandas series of lists.\nvectors = v_context.fit_transform(train_data2['stemmed_context_token'])\ncontext_vector = new_features(vectors, 'ct', v_context)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_vector.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(np.sum(context_vector, axis = 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data3 = pd.concat([train_data2,context_vector],axis=1)\ntrain_data3.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do the same for subject. Now, the tokens are separated by commas in this case. Thus, change them to spaces before tokenizing them.","metadata":{}},{"cell_type":"code","source":"sum(train_data3['subject'].isnull()) #check number of nulls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data3[train_data3['subject'].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two nulls to get rid of. Replace them with \"unknown\".","metadata":{}},{"cell_type":"code","source":"# fill null subject\ntrain_data3[\"subject\"].fillna(value = \"unknown\", inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply defined tokenizer:","metadata":{}},{"cell_type":"code","source":"train_data3['subject'].replace(',',' ',regex=True,inplace=True)  # turn commas to blank spaces\ntrain_data3['stemmed_subject_token'] = tokenize_and_stem(train_data3['subject']) # Process subject strings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check redundant:","metadata":{}},{"cell_type":"code","source":"words_in_subject = countwords(train_data3[\"stemmed_subject_token\"])\nprint(words_in_subject[0:20])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply CountVectorizer:","metadata":{}},{"cell_type":"code","source":"v_subject = CountVectorizer(analyzer=lambda x: x, max_df = 1.0 ,min_df = 50) # need analyzer=lambda x: x to disable the analyzer, since the input is pandas series of lists.\nvectors = v_subject.fit_transform(train_data3['stemmed_subject_token'])\nsubject_vector = new_features(vectors, 'sj', v_subject)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subject_vector.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(np.sum(subject_vector, axis = 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data4 = pd.concat([train_data3,subject_vector],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop original columns and get the final DataFrame:","metadata":{}},{"cell_type":"code","source":"train_data4.drop(columns=['statement', 'stemmed_statement_token','context', 'stemmed_context_token','subject', 'stemmed_subject_token'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data4.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data4.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Data Cleaning and Encoding","metadata":{}},{"cell_type":"code","source":"print(np.sum(train_data4.isnull(),axis = 0)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we ignore the speaker's name and focus on their past speech counts. Even if the counts themselves may not determine if they will speak the truth in the future or not. But in terms of our work, they can serve as vectors determining who is which person. Then, for speaker jobs and their states, we deem that they are less likely to contribute to the model comparing to other atributes. Thus, let's verify if they are really the features to get rid of.","metadata":{}},{"cell_type":"code","source":"print('Speaker\\'s job and states absolute correlation to 6-ways label:', np.abs(train_data4[['speaker_job','state']].corrwith(train_data4['label_6'],method= 'kendall')))\nprint('Speaker\\'s jobs and states absolute correlation to 3-ways label:', np.abs(train_data4[['speaker_job','state']].corrwith(train_data4['label_3'],method= 'kendall')))\nprint('Speaker\\'s jobs and states absolute correlation to 2-ways label:', np.abs(train_data4[['speaker_job','state']].corrwith(train_data4['label_2'],method= 'kendall')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check if there is some individual feartures that are useful.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nonehot = OneHotEncoder(handle_unknown = 'ignore') #ignore tells the encoder to ignore new categories by encoding them with 0's\nvectors = onehot.fit_transform(np.array(train_data4[['speaker_job','state']]))\nspeaker_vectors = new_features(vectors, 'speaker', onehot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Top 5 speaker\\'s job and state absolute correlation to 6-ways label:', np.abs(speaker_vectors.corrwith(train_data4['label_6'],method= 'pearson').nlargest(n=5)))\nprint('Top 5 speaker\\'s job and state absolute correlation to 3-ways label:', np.abs(speaker_vectors.corrwith(train_data4['label_3'],method= 'pearson').nlargest(n=5)))\nprint('Top 5 speaker\\'s job and state absolute correlation to 2-ways label:', np.abs(speaker_vectors.corrwith(train_data4['label_2'],method= 'pearson').nlargest(n=5)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One can see that the correlations to the target class do not exceed 0.034. Also, there is no individual useful feature; thus, confirm our assumption that they are irrelavant. Proceed to drop them together with statement ID and speaker's name.","metadata":{}},{"cell_type":"code","source":"# drop unnescessary columns\ntrain_data4.drop(columns = ['ID','speaker','speaker_job','state'],inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, there are some null for the counts and party:","metadata":{}},{"cell_type":"code","source":"np.sum(train_data4[['pof_count','false_count','barelytrue_count','halftrue_count','mostlytrue_count']].isnull(),axis = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sum(train_data['party'].isnull(),axis = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Assume that these are caused by unknown speaker, simply set them to 0.","metadata":{}},{"cell_type":"code","source":"train_data4['pof_count'].fillna(0, inplace = True)\ntrain_data4['false_count'].fillna(0, inplace = True)\ntrain_data4['barelytrue_count'].fillna(0, inplace = True)\ntrain_data4['halftrue_count'].fillna(0, inplace = True)\ntrain_data4['mostlytrue_count'].fillna(0, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and assume \"unknown\" for the null in party.","metadata":{}},{"cell_type":"code","source":"train_data4['party'].fillna(\"unknown\", inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the party feature correlation to the target classes:","metadata":{}},{"cell_type":"code","source":"print('Speaker\\'s party absolute correlation to 6-ways label:', np.abs(train_data4['party'].corr(train_data4['label_6'],method= 'kendall')))\nprint('Speaker\\'s party absolute correlation to 3-ways label:', np.abs(train_data4['party'].corr(train_data4['label_3'],method= 'kendall')))\nprint('Speaker\\'s party absolute correlation to 2-ways label:', np.abs(train_data4['party'].corr(train_data4['label_2'],method= 'kendall')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Turns out our assumption about the party is wrong. The feature is not as useful as we thought. Nonetheless, let's investigate a little deeper if we can extract any useful feature. First, One-Hot encode party via scikit-learn module:","metadata":{}},{"cell_type":"code","source":"ohe = OneHotEncoder(handle_unknown = 'ignore') #ignore tells the encoder to ignore new categories by encoding them with 0's\nvectors = ohe.fit_transform(np.array(train_data4['party']).reshape(-1, 1))\nparty_vector = new_features(vectors, 'party', ohe)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Top 5 speaker\\'s party absolute correlation to 6-ways label:', np.abs(party_vector.corrwith(train_data4['label_6'],method= 'pearson').nlargest(n=5)))\nprint('Top 5 speaker\\'s party absolute correlation to 3-ways label:', np.abs(party_vector.corrwith(train_data4['label_3'],method= 'pearson').nlargest(n=5)))\nprint('Top 5 speaker\\'s party absolute correlation to 2-ways label:', np.abs(party_vector.corrwith(train_data4['label_2'],method= 'pearson').nlargest(n=5)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Democrat stands out from the other top 5 parties. Therefore, we decide to keep this in our model.","metadata":{}},{"cell_type":"code","source":"train_data4 = pd.concat([train_data4,party_vector['party_x0_democrat']],axis=1)\ntrain_data4.drop(columns=['party'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data4.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training, Valdiation and Test Sets Preparation","metadata":{}},{"cell_type":"markdown","source":"Apply BoW, vectorizer, and one-hot encoding to v and test set without word counts threshold and cut-offs. Then, mask the columns with ones from the training set, such that the columns that do not appear in the training set are dropped. For columns that do not exist in the v and test set, add them and set their values to zero.","metadata":{}},{"cell_type":"code","source":"# fill nulls\nv_data[\"subject\"].fillna(value = \"unknown\", inplace=True)\nv_data[\"context\"].fillna(value = \"unknown\", inplace=True)\nv_data['party'].fillna(\"unknown\", inplace = True)\ntest_data[\"subject\"].fillna(value = \"unknown\", inplace=True)\ntest_data[\"context\"].fillna(value = \"unknown\", inplace=True)\ntest_data['party'].fillna(\"unknown\", inplace = True)\n\nv_data['pof_count'].fillna(0, inplace = True)\nv_data['false_count'].fillna(0, inplace = True)\nv_data['barelytrue_count'].fillna(0, inplace = True)\nv_data['halftrue_count'].fillna(0, inplace = True)\nv_data['mostlytrue_count'].fillna(0, inplace = True)\n\ntest_data['pof_count'].fillna(0, inplace = True)\ntest_data['false_count'].fillna(0, inplace = True)\ntest_data['barelytrue_count'].fillna(0, inplace = True)\ntest_data['halftrue_count'].fillna(0, inplace = True)\ntest_data['mostlytrue_count'].fillna(0, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize statement\nv_data[\"stemmed_statement_token\"] = tokenize_and_stem(v_data['statement'])\ntest_data[\"stemmed_statement_token\"] = tokenize_and_stem(test_data['statement'])\n\n# tokenize context\nv_data[\"stemmed_context_token\"] = tokenize_and_stem(v_data['context'])\ntest_data[\"stemmed_context_token\"] = tokenize_and_stem(test_data['context'])\n\n# turn commas to blank spaces for subjects\nv_data['subject'].replace(',',' ',regex=True,inplace=True) \ntest_data['subject'].replace(',',' ',regex=True,inplace=True)  \n\n# tokenize subject\nv_data[\"stemmed_subject_token\"] = tokenize_and_stem(v_data['subject'])\ntest_data[\"stemmed_subject_token\"] = tokenize_and_stem(test_data['subject'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build n-grams\n\nv_data[\"stemmed_statement_bigrams\"] = v_data[\"stemmed_statement_token\"].apply(build_bigrams)\nv_data[\"stemmed_statement_trigrams\"] = v_data[\"stemmed_statement_token\"].apply(build_trigrams)\n\ntest_data[\"stemmed_statement_bigrams\"] = test_data[\"stemmed_statement_token\"].apply(build_bigrams)\ntest_data[\"stemmed_statement_trigrams\"] = test_data[\"stemmed_statement_token\"].apply(build_trigrams)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply the countvectorizer based on what we have done for the training set via transform.","metadata":{}},{"cell_type":"code","source":"def transformtovec(Pandas_Series, prefixstring, vectorizer):\n    vectors = vectorizer.transform(Pandas_Series)\n    vecdataframe = new_features(vectors, prefixstring, vectorizer)\n    return vecdataframe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statement\n# get new DataFrame\nv_statement_vectors = transformtovec(v_data[\"stemmed_statement_bigrams\"], 'st', v_statement)\ntest_statement_vectors = transformtovec(test_data[\"stemmed_statement_bigrams\"], 'st', v_statement)\n\n# Context\n# get new DataFrame\nv_context_vectors = transformtovec(v_data[\"stemmed_context_token\"], 'ct', v_context)\ntest_context_vectors = transformtovec(test_data[\"stemmed_context_token\"], 'ct', v_context)\n\n# Context\n# get new DataFrame\nv_subject_vectors = transformtovec(v_data[\"stemmed_subject_token\"], 'sj', v_subject)\ntest_subject_vectors = transformtovec(test_data[\"stemmed_subject_token\"], 'sj', v_subject)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge the data in the same order as the training set\n\n# cross validation set\nv_data = pd.concat([v_data,v_statement_vectors],axis=1)\nv_data = pd.concat([v_data,v_context_vectors],axis=1)\nv_data = pd.concat([v_data,v_subject_vectors],axis=1)\n\n# test set\ntest_data = pd.concat([test_data,test_statement_vectors],axis=1)\ntest_data = pd.concat([test_data,test_context_vectors],axis=1)\ntest_data = pd.concat([test_data,test_subject_vectors],axis=1)\n\n# drop original and unnecessary columns\nv_data.drop(columns=['ID','statement', 'stemmed_statement_token','context', 'stemmed_context_token','subject', 'stemmed_subject_token','speaker','speaker_job','state'], inplace=True)\ntest_data.drop(columns=['ID','statement', 'stemmed_statement_token','context', 'stemmed_context_token','subject', 'stemmed_subject_token','speaker','speaker_job','state'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the Democrat One-Hot Feature:","metadata":{}},{"cell_type":"code","source":"# validation\nv_vectors = ohe.transform(np.array(v_data['party']).reshape(-1, 1))\nv_party_vector = new_features(v_vectors, 'party', ohe)\nv_data = pd.concat([v_data,v_party_vector['party_x0_democrat']],axis=1)\nv_data.drop(columns=['party'], inplace=True)\n\n# test\ntest_vectors = ohe.transform(np.array(test_data['party']).reshape(-1, 1))\ntest_party_vector = new_features(test_vectors, 'party', ohe)\ntest_data = pd.concat([test_data,test_party_vector['party_x0_democrat']],axis=1)\ntest_data.drop(columns=['party'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, properly identify the independent and dependent variables.","metadata":{}},{"cell_type":"code","source":"# features\nX_train = train_data4.drop(columns = ['label_2','label_3','label_6'])\nX_v = v_data.drop(columns = ['label_2','label_3','label_6'])\nX_test = test_data.drop(columns = ['label_2','label_3','label_6'])\n\n# targets\ny_train = train_data4[['label_2','label_3','label_6']]\ny_v = v_data[['label_2','label_3','label_6']]\ny_test = test_data[['label_2','label_3','label_6']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = 2 # label to metric: 0,1, 2 for 2, 3 and 6 ways respectively","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayes\n","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train.iloc[:,label])\ny_pred = gnb.predict(X_v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.accuracy_score(y_v['label_6'], y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\nk_range = list(range(1, 250,10))\nscores = []\nscores2 = []\nscore_cv = []\nfor i in k_range:\n    knn = KNeighborsClassifier(n_neighbors=i,weights = 'distance')\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_v)\n    y_pred2 = knn.predict(X_test)\n    cv_scores = cross_val_score(knn, X_train, y_train.iloc[:,label], cv=5)\n    scores.append(metrics.accuracy_score(y_v.iloc[:,label], y_pred[:,label]))\n    score_cv.append(cv_scores.mean())\n    scores2.append(metrics.accuracy_score(y_test.iloc[:,label], y_pred2[:,label]))\n\nplt.plot(k_range, scores, label = 'val')\nplt.plot(k_range, score_cv, label = 'cv 5 fold')\nplt.plot(k_range, scores2, label = 'test')\nplt.legend()\nplt.show()\nprint(np.transpose(np.array([k_range,np.around(scores2,7)*100])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# n = 31\nknn = KNeighborsClassifier(n_neighbors=91,weights = 'distance')\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\ncm = confusion_matrix(y_test.iloc[:,label],y_pred[:,label], labels=knn.classes_[label])\nprint(metrics.accuracy_score(y_test.iloc[:,label], y_pred[:,label]))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=knn.classes_[label])\ndisp.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if plot_knn_accuracy:\n    k_range = list(range(1, 100,5))\n    score_train = []\n    score_v = []\n    score_test = []\n    for i in k_range:\n        knn = KNeighborsClassifier(n_neighbors=i, weights = 'distance',algorithm = 'brute',metric = 'manhattan')\n        knn.fit(X_train, y_train)\n        y_pred_train = knn.predict(X_train)\n        v_scores = cross_val_score(knn, X_train, y_train, v=5)\n        y_pred_test = knn.predict(X_test)\n        score_train.append(metrics.accuracy_score(y_train, y_pred_train))\n        score_v.append(v_scores.mean())\n        score_test.append(metrics.accuracy_score(y_test, y_pred_test))\n    \n    np.array(score_train).tofile('KNN_score_train.csv', sep = ',')   \n    np.array(score_v).tofile('KNN_score_v.csv', sep = ',')   \n    np.array(score_test).tofile('KNN_score_test.csv', sep = ',') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if plot_knn_accuracy:\n    score_train = pd.read_csv(\"/kaggle/working/KNN_score_train.csv\", header = None)\n    score_v = pd.read_csv(\"/kaggle/working/KNN_score_v.csv\", header = None)\n    score_test = pd.read_csv(\"/kaggle/working/KNN_score_test.csv\", header = None)\n\n    #k_range = list(range(1, 60,5))\n\n    plt.plot(k_range, score_train.T ,label = 'train')\n    plt.plot(k_range, score_v.T,label = 'v 5 fold')\n    plt.plot(k_range, score_test.T,label = 'test')\n    plt.legend()\n    plt.xlabel('k')\n    plt.ylabel('Accuracy score')\n    plt.yscale('log')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nif plot_logreg_accuracy:\n    order_range = np.arange(-4,-2,0.2)\n    c_range = 10.0**order_range\n    scores = []\n    for c in c_range:\n        logreg = LogisticRegression(random_state=0,C=c,max_iter = 500)\n        logreg.fit(X_train, y_train)\n        y_pred = logreg.predict(X_v)\n        scores.append(metrics.accuracy_score(y_v, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if plot_logreg_accuracy:\n    plt.plot(order_range, scores)\n    plt.show()\n    print(np.transpose(np.array([order_range,np.around(scores,7)*100])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"train_data4.head(10).filter(regex='statement').columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data4.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data4.head(10).filter(regex='statement')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.pairplot(train_data4[['pof_count','false_count','barelytrue_count','halftrue_count','mostlytrue_count','context_ad','label_2']],hue = 'label_2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from copy import deepcopy\ndf_test = deepcopy(train_data)\ndf_test['subject'] = train_data['subject'].mask(train_data.groupby('subject')['subject'].transform('size').lt(60), 'Others')\ndf_test.drop(df_test[df_test['subject'] ==  'Others'].index, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show unique value for \"context\"","metadata":{}},{"cell_type":"markdown","source":"Show all rows that contain:","metadata":{}},{"cell_type":"code","source":"np.sum(test_data.filter(regex='count').isnull(),axis = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}