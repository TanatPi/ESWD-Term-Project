{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# True-False news classification using LIAR dataset\n### by Tanat Piumsuwan and Worasit Tomduangkaew\n### Chiang Mai University","metadata":{}},{"cell_type":"markdown","source":"This project is mainly to explore basics to NLP and classification proceses, using the LIAR dataset sourced from Politifact website and provided by William Wang Yang back in 2017 [1].\n\nAll credits are given to him for making his dataset widely available.\n\n### Features\n1. ID: statement ID in the from of json file.\n2. label_6: 6-ways fine-grained label ranging from pants-fire, false, barely-true, half-true, mostly-true, and true.\n3. statement\n4. speaker\n5. speaker_job: speaker's job at the time the statement is given.\n6. state: speaker's home state.\n7. party: speaker's affiliated party.\n8. past counts: for how statements given by the speaker, including in this dataset, are labeled in the past.\n9. context: where and how the statement is given.\n\n### Reference\n[1] W. Y. Wang, “‘Liar, Liar Pants on Fire’: A New Benchmark Dataset for Fake News Detection.” arXiv, 2017. doi: 10.48550/ARXIV.1705.00648.\n\n","metadata":{}},{"cell_type":"code","source":"# Important libaries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-09T14:56:02.356760Z","iopub.execute_input":"2023-10-09T14:56:02.358194Z","iopub.status.idle":"2023-10-09T14:56:05.598311Z","shell.execute_reply.started":"2023-10-09T14:56:02.358153Z","shell.execute_reply":"2023-10-09T14:56:05.596977Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Read TSV data\ncolumns = [\"ID\", \"label_6\", \"statement\", \"subject\", \"speaker\", \"speaker_job\", \"state\", \"party\", \"pof_count\", \"false_count\", \"barelytrue_count\", \"halftrue_count\", \"mostlytrue_count\", \"context\"]\ntrain_data = pd.read_table(\"/kaggle/input/LIARbyYang/train.tsv\", header = None, names = columns)\nv_data = pd.read_table(\"/kaggle/input/LIARbyYang/valid.tsv\", header = None, names = columns)\ntest_data = pd.read_table(\"/kaggle/input/LIARbyYang/test.tsv\", header = None, names = columns)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-09T14:56:05.601055Z","iopub.execute_input":"2023-10-09T14:56:05.602155Z","iopub.status.idle":"2023-10-09T14:56:05.795538Z","shell.execute_reply.started":"2023-10-09T14:56:05.602102Z","shell.execute_reply":"2023-10-09T14:56:05.794633Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Here we do not want to merge the three tables provided. Next, check the data:","metadata":{}},{"cell_type":"code","source":"train_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T14:56:05.797045Z","iopub.execute_input":"2023-10-09T14:56:05.797764Z","iopub.status.idle":"2023-10-09T14:56:05.844842Z","shell.execute_reply.started":"2023-10-09T14:56:05.797712Z","shell.execute_reply":"2023-10-09T14:56:05.842984Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"           ID      label_6                                          statement  \\\n0   2635.json        false  Says the Annies List political group supports ...   \n1  10540.json    half-true  When did the decline of coal start? It started...   \n2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n3   1123.json        false  Health care reform legislation is likely to ma...   \n4   9028.json    half-true  The economic turnaround started at the end of ...   \n5  12465.json         true  The Chicago Bears have had more starting quart...   \n6   2342.json  barely-true  Jim Dunnam has not lived in the district he re...   \n7    153.json    half-true  I'm the only person on this stage who has work...   \n8   5602.json    half-true  However, it took $19.5 million in Oregon Lotte...   \n9   9741.json  mostly-true  Says GOP primary opponents Glenn Grothman and ...   \n\n                                     subject                 speaker  \\\n0                                   abortion            dwayne-bohac   \n1         energy,history,job-accomplishments          scott-surovell   \n2                             foreign-policy            barack-obama   \n3                                health-care            blog-posting   \n4                               economy,jobs           charlie-crist   \n5                                  education               robin-vos   \n6                       candidates-biography  republican-party-texas   \n7                                     ethics            barack-obama   \n8                                       jobs          oregon-lottery   \n9  energy,message-machine-2014,voting-record           duey-stroebel   \n\n                  speaker_job      state         party  pof_count  \\\n0        State representative      Texas    republican        0.0   \n1              State delegate   Virginia      democrat        0.0   \n2                   President   Illinois      democrat       70.0   \n3                         NaN        NaN          none        7.0   \n4                         NaN    Florida      democrat       15.0   \n5  Wisconsin Assembly speaker  Wisconsin    republican        0.0   \n6                         NaN      Texas    republican        3.0   \n7                   President   Illinois      democrat       70.0   \n8                         NaN        NaN  organization        0.0   \n9        State representative  Wisconsin    republican        0.0   \n\n   false_count  barelytrue_count  halftrue_count  mostlytrue_count  \\\n0          1.0               0.0             0.0               0.0   \n1          0.0               1.0             1.0               0.0   \n2         71.0             160.0           163.0               9.0   \n3         19.0               3.0             5.0              44.0   \n4          9.0              20.0            19.0               2.0   \n5          3.0               2.0             5.0               1.0   \n6          1.0               1.0             3.0               1.0   \n7         71.0             160.0           163.0               9.0   \n8          0.0               1.0             0.0               1.0   \n9          0.0               0.0             1.0               0.0   \n\n                                    context  \n0                                  a mailer  \n1                           a floor speech.  \n2                                    Denver  \n3                            a news release  \n4                       an interview on CNN  \n5                 a an online opinion-piece  \n6                          a press release.  \n7  a Democratic debate in Philadelphia, Pa.  \n8                                a website   \n9                           an online video  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>label_6</th>\n      <th>statement</th>\n      <th>subject</th>\n      <th>speaker</th>\n      <th>speaker_job</th>\n      <th>state</th>\n      <th>party</th>\n      <th>pof_count</th>\n      <th>false_count</th>\n      <th>barelytrue_count</th>\n      <th>halftrue_count</th>\n      <th>mostlytrue_count</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2635.json</td>\n      <td>false</td>\n      <td>Says the Annies List political group supports ...</td>\n      <td>abortion</td>\n      <td>dwayne-bohac</td>\n      <td>State representative</td>\n      <td>Texas</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>a mailer</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10540.json</td>\n      <td>half-true</td>\n      <td>When did the decline of coal start? It started...</td>\n      <td>energy,history,job-accomplishments</td>\n      <td>scott-surovell</td>\n      <td>State delegate</td>\n      <td>Virginia</td>\n      <td>democrat</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>a floor speech.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>324.json</td>\n      <td>mostly-true</td>\n      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n      <td>foreign-policy</td>\n      <td>barack-obama</td>\n      <td>President</td>\n      <td>Illinois</td>\n      <td>democrat</td>\n      <td>70.0</td>\n      <td>71.0</td>\n      <td>160.0</td>\n      <td>163.0</td>\n      <td>9.0</td>\n      <td>Denver</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1123.json</td>\n      <td>false</td>\n      <td>Health care reform legislation is likely to ma...</td>\n      <td>health-care</td>\n      <td>blog-posting</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>none</td>\n      <td>7.0</td>\n      <td>19.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>44.0</td>\n      <td>a news release</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9028.json</td>\n      <td>half-true</td>\n      <td>The economic turnaround started at the end of ...</td>\n      <td>economy,jobs</td>\n      <td>charlie-crist</td>\n      <td>NaN</td>\n      <td>Florida</td>\n      <td>democrat</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>20.0</td>\n      <td>19.0</td>\n      <td>2.0</td>\n      <td>an interview on CNN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12465.json</td>\n      <td>true</td>\n      <td>The Chicago Bears have had more starting quart...</td>\n      <td>education</td>\n      <td>robin-vos</td>\n      <td>Wisconsin Assembly speaker</td>\n      <td>Wisconsin</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>a an online opinion-piece</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2342.json</td>\n      <td>barely-true</td>\n      <td>Jim Dunnam has not lived in the district he re...</td>\n      <td>candidates-biography</td>\n      <td>republican-party-texas</td>\n      <td>NaN</td>\n      <td>Texas</td>\n      <td>republican</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>a press release.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>153.json</td>\n      <td>half-true</td>\n      <td>I'm the only person on this stage who has work...</td>\n      <td>ethics</td>\n      <td>barack-obama</td>\n      <td>President</td>\n      <td>Illinois</td>\n      <td>democrat</td>\n      <td>70.0</td>\n      <td>71.0</td>\n      <td>160.0</td>\n      <td>163.0</td>\n      <td>9.0</td>\n      <td>a Democratic debate in Philadelphia, Pa.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>5602.json</td>\n      <td>half-true</td>\n      <td>However, it took $19.5 million in Oregon Lotte...</td>\n      <td>jobs</td>\n      <td>oregon-lottery</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>organization</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>a website</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9741.json</td>\n      <td>mostly-true</td>\n      <td>Says GOP primary opponents Glenn Grothman and ...</td>\n      <td>energy,message-machine-2014,voting-record</td>\n      <td>duey-stroebel</td>\n      <td>State representative</td>\n      <td>Wisconsin</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>an online video</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-09T14:56:05.847734Z","iopub.execute_input":"2023-10-09T14:56:05.849176Z","iopub.status.idle":"2023-10-09T14:56:05.858264Z","shell.execute_reply.started":"2023-10-09T14:56:05.849120Z","shell.execute_reply":"2023-10-09T14:56:05.855884Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(10240, 14)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Relabel","metadata":{}},{"cell_type":"markdown","source":"We want to merge the labels from 6-ways to 2 and 3-ways, while keeping the original 6-ways labels for the benchmark.","metadata":{}},{"cell_type":"code","source":"merger_2 = { 'pants-fire' : 0,\n           'false' : 0,\n           'barely-true': 0,\n           'half-true': 0,\n           'mostly-true': 1,\n           'true': 1}\nmerger_3 = { 'pants-fire' : 0,\n           'false' : 0,\n           'barely-true': 1,\n           'half-true': 1,\n           'mostly-true': 2,\n           'true': 2}\nmerger_6 = { 'pants-fire' : 0,\n           'false' : 1,\n           'barely-true': 2,\n           'half-true': 3,\n           'mostly-true': 4,\n           'true': 5}\n\n# training set\ntrain_data['label_2'] = train_data[\"label_6\"].map(merger_2)\ntrain_data['label_3'] = train_data[\"label_6\"].map(merger_3)\ntrain_data[\"label_6\"].replace(merger_6, inplace = True)\n\n# cross validation set\nv_data['label_2'] = v_data[\"label_6\"].map(merger_2)\nv_data['label_3'] = v_data[\"label_6\"].map(merger_3)\nv_data[\"label_6\"].replace(merger_6, inplace = True)\n\n# test set\ntest_data['label_2'] = test_data[\"label_6\"].map(merger_2)\ntest_data['label_3'] = test_data[\"label_6\"].map(merger_3)\ntest_data[\"label_6\"].replace(merger_6, inplace = True)\n\n# check\ntrain_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T14:56:05.859808Z","iopub.execute_input":"2023-10-09T14:56:05.860710Z","iopub.status.idle":"2023-10-09T14:56:05.935634Z","shell.execute_reply.started":"2023-10-09T14:56:05.860660Z","shell.execute_reply":"2023-10-09T14:56:05.934796Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"           ID  label_6                                          statement  \\\n0   2635.json        1  Says the Annies List political group supports ...   \n1  10540.json        3  When did the decline of coal start? It started...   \n2    324.json        4  Hillary Clinton agrees with John McCain \"by vo...   \n3   1123.json        1  Health care reform legislation is likely to ma...   \n4   9028.json        3  The economic turnaround started at the end of ...   \n5  12465.json        5  The Chicago Bears have had more starting quart...   \n6   2342.json        2  Jim Dunnam has not lived in the district he re...   \n7    153.json        3  I'm the only person on this stage who has work...   \n8   5602.json        3  However, it took $19.5 million in Oregon Lotte...   \n9   9741.json        4  Says GOP primary opponents Glenn Grothman and ...   \n\n                                     subject                 speaker  \\\n0                                   abortion            dwayne-bohac   \n1         energy,history,job-accomplishments          scott-surovell   \n2                             foreign-policy            barack-obama   \n3                                health-care            blog-posting   \n4                               economy,jobs           charlie-crist   \n5                                  education               robin-vos   \n6                       candidates-biography  republican-party-texas   \n7                                     ethics            barack-obama   \n8                                       jobs          oregon-lottery   \n9  energy,message-machine-2014,voting-record           duey-stroebel   \n\n                  speaker_job      state         party  pof_count  \\\n0        State representative      Texas    republican        0.0   \n1              State delegate   Virginia      democrat        0.0   \n2                   President   Illinois      democrat       70.0   \n3                         NaN        NaN          none        7.0   \n4                         NaN    Florida      democrat       15.0   \n5  Wisconsin Assembly speaker  Wisconsin    republican        0.0   \n6                         NaN      Texas    republican        3.0   \n7                   President   Illinois      democrat       70.0   \n8                         NaN        NaN  organization        0.0   \n9        State representative  Wisconsin    republican        0.0   \n\n   false_count  barelytrue_count  halftrue_count  mostlytrue_count  \\\n0          1.0               0.0             0.0               0.0   \n1          0.0               1.0             1.0               0.0   \n2         71.0             160.0           163.0               9.0   \n3         19.0               3.0             5.0              44.0   \n4          9.0              20.0            19.0               2.0   \n5          3.0               2.0             5.0               1.0   \n6          1.0               1.0             3.0               1.0   \n7         71.0             160.0           163.0               9.0   \n8          0.0               1.0             0.0               1.0   \n9          0.0               0.0             1.0               0.0   \n\n                                    context  label_2  label_3  \n0                                  a mailer        0        0  \n1                           a floor speech.        0        1  \n2                                    Denver        1        2  \n3                            a news release        0        0  \n4                       an interview on CNN        0        1  \n5                 a an online opinion-piece        1        2  \n6                          a press release.        0        1  \n7  a Democratic debate in Philadelphia, Pa.        0        1  \n8                                a website         0        1  \n9                           an online video        1        2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>label_6</th>\n      <th>statement</th>\n      <th>subject</th>\n      <th>speaker</th>\n      <th>speaker_job</th>\n      <th>state</th>\n      <th>party</th>\n      <th>pof_count</th>\n      <th>false_count</th>\n      <th>barelytrue_count</th>\n      <th>halftrue_count</th>\n      <th>mostlytrue_count</th>\n      <th>context</th>\n      <th>label_2</th>\n      <th>label_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2635.json</td>\n      <td>1</td>\n      <td>Says the Annies List political group supports ...</td>\n      <td>abortion</td>\n      <td>dwayne-bohac</td>\n      <td>State representative</td>\n      <td>Texas</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>a mailer</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10540.json</td>\n      <td>3</td>\n      <td>When did the decline of coal start? It started...</td>\n      <td>energy,history,job-accomplishments</td>\n      <td>scott-surovell</td>\n      <td>State delegate</td>\n      <td>Virginia</td>\n      <td>democrat</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>a floor speech.</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>324.json</td>\n      <td>4</td>\n      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n      <td>foreign-policy</td>\n      <td>barack-obama</td>\n      <td>President</td>\n      <td>Illinois</td>\n      <td>democrat</td>\n      <td>70.0</td>\n      <td>71.0</td>\n      <td>160.0</td>\n      <td>163.0</td>\n      <td>9.0</td>\n      <td>Denver</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1123.json</td>\n      <td>1</td>\n      <td>Health care reform legislation is likely to ma...</td>\n      <td>health-care</td>\n      <td>blog-posting</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>none</td>\n      <td>7.0</td>\n      <td>19.0</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>44.0</td>\n      <td>a news release</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9028.json</td>\n      <td>3</td>\n      <td>The economic turnaround started at the end of ...</td>\n      <td>economy,jobs</td>\n      <td>charlie-crist</td>\n      <td>NaN</td>\n      <td>Florida</td>\n      <td>democrat</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>20.0</td>\n      <td>19.0</td>\n      <td>2.0</td>\n      <td>an interview on CNN</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12465.json</td>\n      <td>5</td>\n      <td>The Chicago Bears have had more starting quart...</td>\n      <td>education</td>\n      <td>robin-vos</td>\n      <td>Wisconsin Assembly speaker</td>\n      <td>Wisconsin</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>a an online opinion-piece</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2342.json</td>\n      <td>2</td>\n      <td>Jim Dunnam has not lived in the district he re...</td>\n      <td>candidates-biography</td>\n      <td>republican-party-texas</td>\n      <td>NaN</td>\n      <td>Texas</td>\n      <td>republican</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>a press release.</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>153.json</td>\n      <td>3</td>\n      <td>I'm the only person on this stage who has work...</td>\n      <td>ethics</td>\n      <td>barack-obama</td>\n      <td>President</td>\n      <td>Illinois</td>\n      <td>democrat</td>\n      <td>70.0</td>\n      <td>71.0</td>\n      <td>160.0</td>\n      <td>163.0</td>\n      <td>9.0</td>\n      <td>a Democratic debate in Philadelphia, Pa.</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>5602.json</td>\n      <td>3</td>\n      <td>However, it took $19.5 million in Oregon Lotte...</td>\n      <td>jobs</td>\n      <td>oregon-lottery</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>organization</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>a website</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9741.json</td>\n      <td>4</td>\n      <td>Says GOP primary opponents Glenn Grothman and ...</td>\n      <td>energy,message-machine-2014,voting-record</td>\n      <td>duey-stroebel</td>\n      <td>State representative</td>\n      <td>Wisconsin</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>an online video</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Bag of Words Process","metadata":{}},{"cell_type":"code","source":"# tokenization processes\nfrom nltk.corpus import stopwords\nimport string\nfrom nltk.tokenize import WhitespaceTokenizer as w_tokenizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nstemmer=SnowballStemmer(\"english\")\nstop_words = set(stopwords.words('english'))\n\n# remove stopwords\ndef remove_stopwords(list_of_tokens):\n    new_list_of_tokens = [w for w in list_of_tokens if not w.lower() in stop_words]\n    return new_list_of_tokens\n\n# remove punctutations and tokens with less than two in length\ndef remove_punctuations_shortwords(list_of_tokens):\n    translation = str.maketrans('', '', string.punctuation)\n    new_list_of_tokens = [tokens.translate(translation) for tokens in list_of_tokens if len(tokens.translate(translation))>=3]\n    return new_list_of_tokens\n\ndef tokenize_and_stem(Pandas_Series):\n    Pandas_Series = Pandas_Series.replace('\\d+', '', regex=True) # remove numbers\n    Pandas_Series = Pandas_Series.apply(nltk.word_tokenize) # tokenize statement\n    Pandas_Series = Pandas_Series.apply(remove_stopwords) # remove stopwords\n    Pandas_Series = Pandas_Series.apply(remove_punctuations_shortwords) # remove punctutations and tokens with less than two in length\n    Pandas_Series = Pandas_Series.apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n    return Pandas_Series","metadata":{"execution":{"iopub.status.busy":"2023-10-09T14:56:05.936935Z","iopub.execute_input":"2023-10-09T14:56:05.937481Z","iopub.status.idle":"2023-10-09T14:56:05.953977Z","shell.execute_reply.started":"2023-10-09T14:56:05.937448Z","shell.execute_reply":"2023-10-09T14:56:05.953030Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_data['stemmed_statement_token'] = tokenize_and_stem(train_data['statement']) # Process statement strings","metadata":{"execution":{"iopub.status.busy":"2023-10-09T14:56:05.955119Z","iopub.execute_input":"2023-10-09T14:56:05.955805Z","iopub.status.idle":"2023-10-09T14:56:10.014872Z","shell.execute_reply.started":"2023-10-09T14:56:05.955758Z","shell.execute_reply":"2023-10-09T14:56:10.013051Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"See sentence length distribution:","metadata":{}},{"cell_type":"code","source":"sns.histplot(train_data['stemmed_statement_token'].str.len())\nplt.xlabel('statement length')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-09T14:56:10.016801Z","iopub.execute_input":"2023-10-09T14:56:10.017269Z","iopub.status.idle":"2023-10-09T14:56:11.110836Z","shell.execute_reply.started":"2023-10-09T14:56:10.017224Z","shell.execute_reply":"2023-10-09T14:56:11.109468Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"One can see that there are some outlier sentences that exceed 50 tokens. Our approach is not to get rid of their records, but to exclude them from the CountVecterizer fitting. To do that, we keep the index of instances with statements having too much tokens.","metadata":{}},{"cell_type":"code","source":"statement_index_tofit = train_data['stemmed_statement_token'].str.len() < 50\ntrain_data[-statement_index_tofit] # instances with very long statements","metadata":{"execution":{"iopub.status.busy":"2023-10-09T14:56:11.111974Z","iopub.execute_input":"2023-10-09T14:56:11.112289Z","iopub.status.idle":"2023-10-09T14:56:11.146522Z","shell.execute_reply.started":"2023-10-09T14:56:11.112263Z","shell.execute_reply":"2023-10-09T14:56:11.145367Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"             ID  label_6                                          statement  \\\n1280  1606.json        4  Hospitals, doctors, MRIs, surgeries and so for...   \n6119  1993.json        5  Georgia has the most restrictive ballot access...   \n7550  1720.json        1  The vast majority of the money I got was from ...   \n\n                      subject        speaker            speaker_job  \\\n1280  elections,ethics,states  charlie-crist                    NaN   \n6119                    crime     dan-gelber                    NaN   \n7550  stimulus,transportation   bobby-jindal  Governor of Louisiana   \n\n          state       party  pof_count  false_count  barelytrue_count  \\\n1280    Florida    democrat       15.0          9.0              20.0   \n6119    Florida    democrat        2.0          2.0               1.0   \n7550  Louisiana  republican        0.0          1.0               4.0   \n\n      halftrue_count  mostlytrue_count  \\\n1280            19.0               2.0   \n6119             3.0               0.0   \n7550             4.0               0.0   \n\n                                                context  label_2  label_3  \\\n1280                        a debate on Fox News Sunday        1        2   \n6119                                  his campaign blog        1        2   \n7550  the Republican response to Obama's speech to C...        0        0   \n\n                                stemmed_statement_token  \n1280  [hospit, doctor, mris, surgeri, forth, extens,...  \n6119  [georgia, restrict, ballot, access, law, count...  \n7550  [vast, major, money, got, small, donor, across...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>label_6</th>\n      <th>statement</th>\n      <th>subject</th>\n      <th>speaker</th>\n      <th>speaker_job</th>\n      <th>state</th>\n      <th>party</th>\n      <th>pof_count</th>\n      <th>false_count</th>\n      <th>barelytrue_count</th>\n      <th>halftrue_count</th>\n      <th>mostlytrue_count</th>\n      <th>context</th>\n      <th>label_2</th>\n      <th>label_3</th>\n      <th>stemmed_statement_token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1280</th>\n      <td>1606.json</td>\n      <td>4</td>\n      <td>Hospitals, doctors, MRIs, surgeries and so for...</td>\n      <td>elections,ethics,states</td>\n      <td>charlie-crist</td>\n      <td>NaN</td>\n      <td>Florida</td>\n      <td>democrat</td>\n      <td>15.0</td>\n      <td>9.0</td>\n      <td>20.0</td>\n      <td>19.0</td>\n      <td>2.0</td>\n      <td>a debate on Fox News Sunday</td>\n      <td>1</td>\n      <td>2</td>\n      <td>[hospit, doctor, mris, surgeri, forth, extens,...</td>\n    </tr>\n    <tr>\n      <th>6119</th>\n      <td>1993.json</td>\n      <td>5</td>\n      <td>Georgia has the most restrictive ballot access...</td>\n      <td>crime</td>\n      <td>dan-gelber</td>\n      <td>NaN</td>\n      <td>Florida</td>\n      <td>democrat</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>his campaign blog</td>\n      <td>1</td>\n      <td>2</td>\n      <td>[georgia, restrict, ballot, access, law, count...</td>\n    </tr>\n    <tr>\n      <th>7550</th>\n      <td>1720.json</td>\n      <td>1</td>\n      <td>The vast majority of the money I got was from ...</td>\n      <td>stimulus,transportation</td>\n      <td>bobby-jindal</td>\n      <td>Governor of Louisiana</td>\n      <td>Louisiana</td>\n      <td>republican</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>the Republican response to Obama's speech to C...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[vast, major, money, got, small, donor, across...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Apply CountVectorizer, ignore the first few rendundant words and words that appear only once.","metadata":{}},{"cell_type":"code","source":"# untokenized the tokens first\n# this is done due to some conflict with the CountVectorizer arguement regarding its analyzer\n# otherwise CountVectorizer ngrams could not be used, making the task complicated\ntrain_data[\"stemmed_statement_token\"] = train_data[\"stemmed_statement_token\"].apply(TreebankWordDetokenizer().detokenize) ","metadata":{"execution":{"iopub.status.busy":"2023-10-09T14:56:11.150019Z","iopub.execute_input":"2023-10-09T14:56:11.150375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n \nv_statement = CountVectorizer(analyzer= 'word', max_df = 1.0 ,min_df = 2, ngram_range = (2,2)) # need analyzer=lambda x: x to disable the analyzer, since the input is pandas series of lists.\n# without cutting out via correlation, it is found that bigram at min_df=2 performs the best\nv_statement.fit(train_data[\"stemmed_statement_token\"][statement_index_tofit])\nvectors = v_statement.transform(train_data[\"stemmed_statement_token\"])\n\ndef new_features(X, string, vectorizer):\n    new_features = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names_out()) # Get new features from countvectorizer\n    if type(new_features.columns[0]) == tuple:\n        new_features.rename(columns='_'.join, inplace=True)\n    new_features.columns = string + '_' + new_features.columns # add prefix to the column names\n    return new_features\n\nstatement_vector = new_features(vectors, 'st', v_statement)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"statement_vector.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.abs(statement_vector.corrwith(train_data['label_6'],method= 'pearson')).nlargest(n=10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(np.abs(statement_vector.corrwith(train_data['label_6'],method= 'pearson')))\nplt.xlabel('correlation')\nplt.title('Statement ngrams correlation to the 6-ways labels')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After some investigation, not cutting out further features via correlation yields better result.","metadata":{}},{"cell_type":"code","source":"statement_corr_cut = False\nif statement_corr_cut:\n    statement_corr_threshold = 0.0002 # the correlation threshold\n    statement_mask = np.abs(statement_vector.corrwith(train_data['label_6'],method= 'pearson'))>statement_corr_threshold\n    statement_vector = statement_vector.loc[:,statement_mask]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Merge new features","metadata":{}},{"cell_type":"code","source":"train_data2 = pd.concat([train_data,statement_vector],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do the same for context. But there are 102 null present, drop them first.","metadata":{}},{"cell_type":"code","source":"sum(train_data2['context'].isnull())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Suppose null value for the context is caused by unknown source, replace them with \"unknown\".","metadata":{}},{"cell_type":"code","source":"# fill null context\ntrain_data2[\"context\"].fillna(value = \"unknown\", inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data2['stemmed_context_token'] = tokenize_and_stem(train_data2['context']) # Process context strings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(train_data2['stemmed_context_token'].str.len())\nplt.xlabel('sentence length')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply vectorizer for contexts containing less than 9 tokens:","metadata":{}},{"cell_type":"code","source":"context_index_tofit = train_data2['stemmed_context_token'].str.len() < 9","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# untokenized the tokens first\n# this is done due to some conflict with the CountVectorizer arguement regarding its analyzer\n# otherwise CountVectorizer ngrams could not be used, making the task complicated\ntrain_data2[\"stemmed_context_token\"] = train_data2[\"stemmed_context_token\"].apply(TreebankWordDetokenizer().detokenize) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v_context = CountVectorizer(analyzer='word', max_df = 1.0 ,min_df = 2, ngram_range = (2,2)) # need analyzer=lambda x: x to disable the analyzer, since the input is pandas series of lists.\n# without cutting out via correlation, it is found that 1gram at min_df=2  performs the best\nv_context.fit(train_data2['stemmed_context_token'][context_index_tofit])\nvectors = v_context.transform(train_data2['stemmed_context_token'])\ncontext_vector = new_features(vectors, 'ct', v_context)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_vector.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.abs(context_vector.corrwith(train_data2['label_6'],method= 'pearson')).nlargest(n=10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(np.abs(context_vector.corrwith(train_data2['label_6'],method= 'pearson')))\nplt.xlabel('correlation')\nplt.title('Context ngrams correlation to the 6-ways labels')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_corr_cut = True\nif context_corr_cut:\n    context_corr_threshold = 0.1 # the correlation threshold\n    context_mask = np.abs(context_vector.corrwith(train_data2['label_6'],method= 'pearson'))>context_corr_threshold\n    context_vector = context_vector.loc[:,context_mask]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data3 = pd.concat([train_data2,context_vector],axis=1)\ntrain_data3.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do the same for subject. Now, the tokens are separated by commas in this case. Thus, change them to spaces before tokenizing them.","metadata":{}},{"cell_type":"code","source":"sum(train_data3['subject'].isnull()) #check number of nulls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data3[train_data3['subject'].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two nulls to get rid of. Replace them with \"unknown\".","metadata":{}},{"cell_type":"code","source":"# fill null subject\ntrain_data3[\"subject\"].fillna(value = \"unknown\", inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply defined tokenizer:","metadata":{}},{"cell_type":"code","source":"train_data3['subject'].replace(',',' ',regex=True,inplace=True)  # turn commas to blank spaces\ntrain_data3['stemmed_subject_token'] = tokenize_and_stem(train_data3['subject']) # Process subject strings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply CountVectorizer:","metadata":{}},{"cell_type":"code","source":"v_subject = CountVectorizer(analyzer=lambda x: x, max_df = 1.0 ,min_df = 1) # need analyzer=lambda x: x to disable the analyzer, since the input is pandas series of lists.\nvectors = v_subject.fit_transform(train_data3['stemmed_subject_token'])\nsubject_vector = new_features(vectors, 'sj', v_subject)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subject_vector.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.abs(subject_vector.corrwith(train_data3['label_6'],method= 'pearson')).nlargest(n=10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(np.abs(subject_vector.corrwith(train_data3['label_6'],method= 'pearson')))\nplt.xlabel('correlation')\nplt.title('subject tokens correlation to the 6-ways labels')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subject_corr_cut = True\nif subject_corr_cut:\n    subject_corr_threshold = 0.06 # the correlation threshold\n    subject_mask = np.abs(subject_vector.corrwith(train_data3['label_6'],method= 'pearson'))>subject_corr_threshold\n    subject_vector = subject_vector.loc[:,subject_mask]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data4 = pd.concat([train_data3,subject_vector],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop original columns and get the final DataFrame:","metadata":{}},{"cell_type":"code","source":"train_data4.drop(columns=['statement', 'stemmed_statement_token','context', 'stemmed_context_token','subject', 'stemmed_subject_token'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data4.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data4.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Data Cleaning and Encoding","metadata":{}},{"cell_type":"code","source":"print(np.sum(train_data4.isnull(),axis = 0)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we ignore the speaker's name and focus on their past speech counts. Even if the counts themselves may not determine if they will speak the truth in the future or not. But in terms of our work, they can serve as vectors determining who is which person. Then, for speaker jobs and their states, we deem that they are less likely to contribute to the model comparing to other atributes. Thus, let's verify if they are really the features to get rid of.","metadata":{}},{"cell_type":"code","source":"# fill null for speaker's information with 'unknown'\n\ntrain_data4['speaker_job'].fillna('unknown', inplace = True)\ntrain_data4['state'].fillna('unknown', inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Speaker\\'s job and states absolute correlation to 6-ways label:', np.abs(train_data4[['speaker_job','state']].corrwith(train_data4['label_6'],method= 'kendall')))\nprint('Speaker\\'s jobs and states absolute correlation to 3-ways label:', np.abs(train_data4[['speaker_job','state']].corrwith(train_data4['label_3'],method= 'kendall')))\nprint('Speaker\\'s jobs and states absolute correlation to 2-ways label:', np.abs(train_data4[['speaker_job','state']].corrwith(train_data4['label_2'],method= 'kendall')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check if there is some individual feartures that are useful.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nonehot = OneHotEncoder(handle_unknown = 'ignore') #ignore tells the encoder to ignore new categories by encoding them with 0's\nvectors = onehot.fit_transform(np.array(train_data4[['speaker_job','state']]))\nspeaker_vectors = new_features(vectors, 'speaker', onehot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(np.abs(speaker_vectors.corrwith(train_data4['label_6'],method= 'pearson')))\nplt.xlabel('correlation')\nplt.title('Speaker\\'s info correlation to the 6-ways labels')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Top 10 speaker\\'s info absolute correlation to 6-ways label:', np.abs(speaker_vectors.corrwith(train_data4['label_6'],method= 'pearson')).nlargest(n=10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One can see that the correlations to the target class do not exceed 0.034; thus, confirm our assumption that they are irrelavant. However, there are a few outlier to be kept. Proceed to some columns with correlation threshold and the original columns, together with statement ID and speaker's name.","metadata":{}},{"cell_type":"code","source":"keepspeaker_info = True\nif keepspeaker_info:\n    speaker_info_corr_threshold = 0.05 # the correlation threshold\n    train_data4 = pd.concat([train_data4,speaker_vectors.loc[:,np.abs(speaker_vectors.corrwith(train_data4['label_6'],method= 'pearson'))>speaker_info_corr_threshold] ],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop unnescessary columns\ntrain_data4.drop(columns = ['ID','speaker','speaker_job','state'],inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, there are some null for the counts and party:","metadata":{}},{"cell_type":"code","source":"np.sum(train_data4[['pof_count','false_count','barelytrue_count','halftrue_count','mostlytrue_count']].isnull(),axis = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sum(train_data['party'].isnull(),axis = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Assume that these are caused by unknown speaker, simply set them to 0.","metadata":{}},{"cell_type":"code","source":"train_data4['pof_count'].fillna(0, inplace = True)\ntrain_data4['false_count'].fillna(0, inplace = True)\ntrain_data4['barelytrue_count'].fillna(0, inplace = True)\ntrain_data4['halftrue_count'].fillna(0, inplace = True)\ntrain_data4['mostlytrue_count'].fillna(0, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and assume \"unknown\" for the null in party.","metadata":{}},{"cell_type":"code","source":"train_data4['party'].fillna(\"unknown\", inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the party feature correlation to the target classes:","metadata":{}},{"cell_type":"code","source":"print('Speaker\\'s party absolute correlation to 6-ways label:', np.abs(train_data4['party'].corr(train_data4['label_6'],method= 'kendall')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Turns out our assumption about the party is wrong. The feature is not as useful as we thought. Nonetheless, let's investigate a little deeper if we can extract any useful feature. First, One-Hot encode party via scikit-learn module:","metadata":{}},{"cell_type":"code","source":"ohe = OneHotEncoder(handle_unknown = 'ignore') #ignore tells the encoder to ignore new categories by encoding them with 0's\nvectors = ohe.fit_transform(np.array(train_data4['party']).reshape(-1, 1))\nparty_vector = new_features(vectors, 'party', ohe)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Top 5 speaker\\'s party absolute correlation to 6-ways label:', np.abs(party_vector.corrwith(train_data4['label_6'],method= 'pearson')).nlargest(n=5))\nprint('Top 5 speaker\\'s party absolute correlation to 3-ways label:', np.abs(party_vector.corrwith(train_data4['label_3'],method= 'pearson')).nlargest(n=5))\nprint('Top 5 speaker\\'s party absolute correlation to 2-ways label:', np.abs(party_vector.corrwith(train_data4['label_2'],method= 'pearson')).nlargest(n=5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(np.abs(party_vector.corrwith(train_data4['label_6'],method= 'pearson')))\nplt.xlabel('correlation')\nplt.title('Party correlation to the 6-ways labels')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Democrat and Republican stand out from the other top 5 parties. Therefore, we decide to keep this in our model.","metadata":{}},{"cell_type":"code","source":"party_corr_cut = True\nif party_corr_cut:\n    party_corr_threshold = 0.08 # the correlation threshold\n    party_mask = np.abs(party_vector.corrwith(train_data4['label_6'],method= 'pearson'))>party_corr_threshold\n    party_vector = party_vector.loc[:,party_mask]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data4 = pd.concat([train_data4,party_vector],axis=1)\ntrain_data4.drop(columns=['party'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data4.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training, Valdiation and Test Sets Preparation","metadata":{}},{"cell_type":"markdown","source":"Apply BoW, vectorizer, and one-hot encoding to v and test set without word counts threshold and cut-offs. Then, mask the columns with ones from the training set, such that the columns that do not appear in the training set are dropped. For columns that do not exist in the v and test set, add them and set their values to zero.","metadata":{}},{"cell_type":"code","source":"# fill nulls\nv_data[\"subject\"].fillna(value = \"unknown\", inplace=True)\nv_data[\"context\"].fillna(value = \"unknown\", inplace=True)\nv_data['party'].fillna(\"unknown\", inplace = True)\ntest_data[\"subject\"].fillna(value = \"unknown\", inplace=True)\ntest_data[\"context\"].fillna(value = \"unknown\", inplace=True)\ntest_data['party'].fillna(\"unknown\", inplace = True)\n\nv_data['pof_count'].fillna(0, inplace = True)\nv_data['false_count'].fillna(0, inplace = True)\nv_data['barelytrue_count'].fillna(0, inplace = True)\nv_data['halftrue_count'].fillna(0, inplace = True)\nv_data['mostlytrue_count'].fillna(0, inplace = True)\n\ntest_data['pof_count'].fillna(0, inplace = True)\ntest_data['false_count'].fillna(0, inplace = True)\ntest_data['barelytrue_count'].fillna(0, inplace = True)\ntest_data['halftrue_count'].fillna(0, inplace = True)\ntest_data['mostlytrue_count'].fillna(0, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize statement\nv_data[\"stemmed_statement_token\"] = tokenize_and_stem(v_data['statement'])\nv_data[\"stemmed_statement_token\"] = v_data[\"stemmed_statement_token\"].apply(TreebankWordDetokenizer().detokenize) \ntest_data[\"stemmed_statement_token\"] = tokenize_and_stem(test_data['statement'])\ntest_data[\"stemmed_statement_token\"] = test_data[\"stemmed_statement_token\"].apply(TreebankWordDetokenizer().detokenize)\n\n# tokenize context\nv_data[\"stemmed_context_token\"] = tokenize_and_stem(v_data['context'])\nv_data[\"stemmed_context_token\"] = v_data[\"stemmed_context_token\"].apply(TreebankWordDetokenizer().detokenize) \ntest_data[\"stemmed_context_token\"] = tokenize_and_stem(test_data['context'])\ntest_data[\"stemmed_context_token\"] = test_data[\"stemmed_context_token\"].apply(TreebankWordDetokenizer().detokenize) \n\n# turn commas to blank spaces for subjects\nv_data['subject'].replace(',',' ',regex=True,inplace=True) \ntest_data['subject'].replace(',',' ',regex=True,inplace=True)  \n\n# tokenize subject\nv_data[\"stemmed_subject_token\"] = tokenize_and_stem(v_data['subject'])\ntest_data[\"stemmed_subject_token\"] = tokenize_and_stem(test_data['subject'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply the countvectorizer based on what we have done for the training set via transform.","metadata":{}},{"cell_type":"code","source":"def transformtovec(Pandas_Series, prefixstring, vectorizer):\n    vectors = vectorizer.transform(Pandas_Series)\n    vecdataframe = new_features(vectors, prefixstring, vectorizer)\n    return vecdataframe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statement\n# get new DataFrame\nv_statement_vectors = transformtovec(v_data[\"stemmed_statement_token\"], 'st', v_statement)\ntest_statement_vectors = transformtovec(test_data[\"stemmed_statement_token\"], 'st', v_statement)\n\nif statement_corr_cut:\n    v_statement_vectors = v_statement_vectors.loc[:,statement_mask]\n    test_statement_vectors = test_statement_vectors.loc[:,statement_mask]\n\n# Context\n# get new DataFrame\nv_context_vectors = transformtovec(v_data[\"stemmed_context_token\"], 'ct', v_context)\ntest_context_vectors = transformtovec(test_data[\"stemmed_context_token\"], 'ct', v_context)\n\nif context_corr_cut:\n    v_context_vectors = v_context_vectors.loc[:,context_mask]\n    test_context_vectors = test_context_vectors.loc[:,context_mask]\n\n# Context\n# get new DataFrame\nv_subject_vectors = transformtovec(v_data[\"stemmed_subject_token\"], 'sj', v_subject)\ntest_subject_vectors = transformtovec(test_data[\"stemmed_subject_token\"], 'sj', v_subject)\n\nif subject_corr_cut:\n    v_subject_vectors = v_subject_vectors.loc[:,subject_mask]\n    test_subject_vectors = test_subject_vectors.loc[:,subject_mask]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge the data in the same order as the training set\n\n# cross validation set\nv_data = pd.concat([v_data,v_statement_vectors],axis=1)\nv_data = pd.concat([v_data,v_context_vectors],axis=1)\nv_data = pd.concat([v_data,v_subject_vectors],axis=1)\n\n# test set\ntest_data = pd.concat([test_data,test_statement_vectors],axis=1)\ntest_data = pd.concat([test_data,test_context_vectors],axis=1)\ntest_data = pd.concat([test_data,test_subject_vectors],axis=1)\n\n# drop original and unnecessary columns\nv_data.drop(columns=['ID','statement', 'stemmed_statement_token','context', 'stemmed_context_token','subject', 'stemmed_subject_token','speaker'], inplace=True)\ntest_data.drop(columns=['ID','statement', 'stemmed_statement_token','context', 'stemmed_context_token','subject', 'stemmed_subject_token','speaker'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get speaker information vectors","metadata":{}},{"cell_type":"code","source":"if keepspeaker_info:\n    # validation\n    vectors = onehot.transform(np.array(v_data[['speaker_job','state']]))\n    v_speaker_vectors = new_features(vectors, 'speaker', onehot)\n    v_data = pd.concat([v_data,v_speaker_vectors.loc[:,np.abs(speaker_vectors.corrwith(train_data4['label_6'],method= 'pearson'))>speaker_info_corr_threshold] ],axis=1)\n\n    # test\n    vectors = onehot.transform(np.array(test_data[['speaker_job','state']]))\n    test_speaker_vectors = new_features(vectors, 'speaker', onehot)\n    test_data = pd.concat([test_data,test_speaker_vectors.loc[:,np.abs(speaker_vectors.corrwith(train_data4['label_6'],method= 'pearson'))>speaker_info_corr_threshold] ],axis=1)\n\n# drop original and unnecessary columns\nv_data.drop(columns=['speaker_job','state'], inplace=True)\ntest_data.drop(columns=['speaker_job','state'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the Democrat One-Hot Feature:","metadata":{}},{"cell_type":"code","source":"# validation\nvectors = ohe.transform(np.array(v_data['party']).reshape(-1, 1))\nv_party_vector = new_features(vectors, 'party', ohe)\nv_data.drop(columns=['party'], inplace=True)\n\n# test\nvectors = ohe.transform(np.array(test_data['party']).reshape(-1, 1))\ntest_party_vector = new_features(vectors, 'party', ohe)\ntest_data.drop(columns=['party'], inplace=True)\n\nif party_corr_cut:\n    v_party_vector = v_party_vector.loc[:,party_mask]\n    test_party_vector = test_party_vector.loc[:,party_mask]\n\nv_data = pd.concat([v_data,v_party_vector],axis = 1)\ntest_data = pd.concat([test_data,test_party_vector],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, properly identify the independent and dependent variables.","metadata":{}},{"cell_type":"code","source":"# features\nX_train = train_data4.drop(columns = ['label_2','label_3','label_6'])\nX_v = v_data.drop(columns = ['label_2','label_3','label_6'])\nX_test = test_data.drop(columns = ['label_2','label_3','label_6'])\n\n# targets\ny_train = train_data4[['label_2','label_3','label_6']]\ny_v = v_data[['label_2','label_3','label_6']]\ny_test = test_data[['label_2','label_3','label_6']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"Generate three types of model from the data.","metadata":{}},{"cell_type":"code","source":"label = 2 # label to metric: 0,1, 2 for 2, 3 and 6 ways respectively","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dropctfeature = False # drop subject for model testing\ndropsjfeature = False # drop subject for model testing\n\nprint(X_train.shape)\nif dropctfeature:\n    drop = list(X_train.filter(regex='ct'))\n    X_train.drop(columns = drop, inplace = True)\n    X_v.drop(columns = drop, inplace = True)\n    X_test.drop(columns = drop, inplace = True)\n\nif dropsjfeature:\n    drop = list(X_train.filter(regex='sj'))\n    X_train.drop(columns = drop, inplace = True)\n    X_v.drop(columns = drop, inplace = True)\n    X_test.drop(columns = drop, inplace = True)\nprint(X_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes\n","metadata":{}},{"cell_type":"code","source":"BuildNB = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nif BuildNB:\n    gnb = GaussianNB()\n    gnb.fit(X_train, y_train.iloc[:,label])\n    y_pred_train = gnb.predict(X_train)\n    cv_score = cross_val_score(gnb, X_train, y_train.iloc[:,label], cv=5)\n    y_pred_v = gnb.predict(X_v)\n    y_pred_test = gnb.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nif BuildNB:\n    print('training :', metrics.accuracy_score(y_train.iloc[:,label], y_pred_train))\n    print('cv 5 fold :', cv_score.mean())\n    print('validation :', metrics.accuracy_score(y_v.iloc[:,label], y_pred_v))\n    print('test :', metrics.accuracy_score(y_test.iloc[:,label], y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Naive Bayes model performs very bad.","metadata":{}},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"code","source":"BuildKNN = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nif BuildKNN:\n    k_range = list(range(1, 182,20))\n    scores_train = []\n    scores = []\n    scores2 = []\n    score_cv = []\n    for i in k_range:\n        knn = KNeighborsClassifier(n_neighbors=i,weights = 'distance')\n        knn.fit(X_train, y_train)\n        y_pred_t = knn.predict(X_train)\n        y_pred = knn.predict(X_v)\n        y_pred2 = knn.predict(X_test)\n        cv_scores = cross_val_score(knn, X_train, y_train.iloc[:,label], cv=5)\n        scores_train.append(metrics.accuracy_score(y_train.iloc[:,label], y_pred_t[:,label]))\n        scores.append(metrics.accuracy_score(y_v.iloc[:,label], y_pred[:,label]))\n        score_cv.append(cv_scores.mean())\n        scores2.append(metrics.accuracy_score(y_test.iloc[:,label], y_pred2[:,label]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if BuildKNN:\n    plt.plot(k_range, scores_train, label = 'train')\n    plt.plot(k_range, score_cv, label = 'cv 5 fold')\n    plt.plot(k_range, scores, label = 'validation')\n    plt.plot(k_range, scores2, label = 'test')\n    plt.legend()\n    plt.xlabel('k')\n    plt.ylabel('Accuracy')\n    plt.show()\n    print(np.transpose(np.array([k_range,np.around(scores2,7)*100])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if BuildKNN:\n    # scale down the range to find the most optimal k\n    k_range = list(range(70, 90,1))\n    scores_train = []\n    scores = []\n    scores2 = []\n    score_cv = []\n    for i in k_range:\n        knn = KNeighborsClassifier(n_neighbors=i,weights = 'distance')\n        knn.fit(X_train, y_train)\n        y_pred_t = knn.predict(X_train)\n        y_pred = knn.predict(X_v)\n        y_pred2 = knn.predict(X_test)\n        cv_scores = cross_val_score(knn, X_train, y_train.iloc[:,label], cv=5)\n        scores_train.append(metrics.accuracy_score(y_train.iloc[:,label], y_pred_t[:,label]))\n        scores.append(metrics.accuracy_score(y_v.iloc[:,label], y_pred[:,label]))\n        score_cv.append(cv_scores.mean())\n        scores2.append(metrics.accuracy_score(y_test.iloc[:,label], y_pred2[:,label]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if BuildKNN:\n    plt.plot(k_range, scores_train, label = 'train')\n    plt.plot(k_range, score_cv, label = 'cv 5 fold')\n    plt.plot(k_range, scores, label = 'validation')\n    plt.plot(k_range, scores2, label = 'test')\n    plt.legend()\n    plt.xlabel('k')\n    plt.ylabel('Accuracy')\n    plt.show()\n    print(np.transpose(np.array([k_range,np.around(scores2,7)*100])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n# best k at k =87\nif BuildKNN:\n    knn = KNeighborsClassifier(n_neighbors=87,weights = 'distance')\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    cm = confusion_matrix(y_test.iloc[:,label],y_pred[:,label], labels=knn.classes_[label])\n    print(metrics.accuracy_score(y_test.iloc[:,label], y_pred[:,label]))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=knn.classes_[label])\n    disp.plot()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"BuildLR = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nif BuildLR:\n    order_range = np.arange(-4,1,1)\n    c_range = 10.0**order_range\n    scoreslr_train = []\n    scoreslr = []\n    scoreslr2 = []\n    score_cvlr = []\n    for c in c_range:\n        logreg = LogisticRegression(random_state=0,C=c,max_iter = 3000)\n        logreg.fit(X_train, y_train.iloc[:,label])\n        y_pred_t = logreg.predict(X_train)\n        y_pred = logreg.predict(X_test)\n        y_pred_v = logreg.predict(X_v)\n        cv_scores = cross_val_score(logreg, X_train, y_train.iloc[:,label], cv=2)\n        scoreslr_train.append(metrics.accuracy_score(y_train.iloc[:,label], y_pred_t))\n        scoreslr.append(metrics.accuracy_score(y_test.iloc[:,label], y_pred))\n        scoreslr2.append(metrics.accuracy_score(y_v.iloc[:,label], y_pred_v))\n        score_cvlr.append(cv_scores.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if BuildLR:\n    plt.plot(order_range, scoreslr_train, label = 'train')\n    plt.plot(order_range, score_cvlr, label = 'cv 2 fold')\n    plt.plot(order_range, scoreslr2, label = 'validation')\n    plt.plot(order_range, scoreslr, label = 'test')\n    plt.xlabel('c')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n    print(np.transpose(np.array([order_range,np.around(scoreslr,7)*100])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not just we encounter non convergence problem for this model. Best Logistic Regression model at around c = 10^(-3) could not even come close to the KNN model. This model is hard to be optimize possibly because the huge amount of features.","metadata":{}},{"cell_type":"markdown","source":"# Final KNN Model\n\nConsider that there are some highly correlated columns from states included in the features. While in fact, discrimating someone from any state that they tend to lie more does not make sense. Thus, we will remove them to see if there is any improvement.","metadata":{}},{"cell_type":"code","source":"X_train.filter(regex='speaker_x1') # there are columns for speaker from ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop state columns\ndrop = list(X_train.filter(regex='speaker_x1'))\nX_train2 = X_train.drop(columns = drop)\nX_v2 = X_v.drop(columns = drop)\nX_test2 = X_test.drop(columns = drop)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BuildfinalKNN = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if BuildfinalKNN:\n    k_range = list(range(1, 182,20))\n    scores_train_final = []\n    scores_final = []\n    scores2_final = []\n    score_cv_final = []\n    for i in k_range:\n        knn = KNeighborsClassifier(n_neighbors=i,weights = 'distance')\n        knn.fit(X_train2, y_train)\n        y_pred_t = knn.predict(X_train2)\n        y_pred = knn.predict(X_v2)\n        y_pred2 = knn.predict(X_test2)\n        cv_scores = cross_val_score(knn, X_train2, y_train.iloc[:,label], cv=5)\n        scores_train_final.append(metrics.accuracy_score(y_train.iloc[:,label], y_pred_t[:,label]))\n        scores_final.append(metrics.accuracy_score(y_v.iloc[:,label], y_pred[:,label]))\n        score_cv_final.append(cv_scores.mean())\n        scores2_final.append(metrics.accuracy_score(y_test.iloc[:,label], y_pred2[:,label]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if BuildfinalKNN:\n    plt.plot(k_range, scores_train_final, label = 'train')\n    plt.plot(k_range, score_cv_final, label = 'cv 5 fold')\n    plt.plot(k_range, scores_final, label = 'validation')\n    plt.plot(k_range, scores2_final, label = 'test')\n    plt.legend()\n    plt.xlabel('k')\n    plt.ylabel('Accuracy')\n    plt.show()\n    print(np.transpose(np.array([k_range,np.around(scores2_final,7)*100])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if BuildfinalKNN:\n    # scale down the range to find the most optimum k\n    k_range = list(range(70, 90, 1))\n    scores_train_final = []\n    scores_final = []\n    scores2_final = []\n    score_cv_final = []\n    for i in k_range:\n        knn = KNeighborsClassifier(n_neighbors=i,weights = 'distance')\n        knn.fit(X_train2, y_train)\n        y_pred_t = knn.predict(X_train2)\n        y_pred = knn.predict(X_v2)\n        y_pred2 = knn.predict(X_test2)\n        cv_scores = cross_val_score(knn, X_train2, y_train.iloc[:,label], cv=5)\n        scores_train_final.append(metrics.accuracy_score(y_train.iloc[:,label], y_pred_t[:,label]))\n        scores_final.append(metrics.accuracy_score(y_v.iloc[:,label], y_pred[:,label]))\n        score_cv_final.append(cv_scores.mean())\n        scores2_final.append(metrics.accuracy_score(y_test.iloc[:,label], y_pred2[:,label]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if BuildfinalKNN:\n    plt.plot(k_range, scores_train_final, label = 'train')\n    plt.plot(k_range, score_cv_final, label = 'cv 5 fold')\n    plt.plot(k_range, scores_final, label = 'validation')\n    plt.plot(k_range, scores2_final, label = 'test')\n    plt.legend()\n    plt.xlabel('k')\n    plt.ylabel('Accuracy')\n    plt.show()\n    print(np.transpose(np.array([k_range,np.around(scores2_final,7)*100])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if BuildfinalKNN: \n    # old best k at k = 87\n    knn = KNeighborsClassifier(n_neighbors=87,weights = 'distance')\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    y_pred2 = knn.predict(X_v)\n    y_pred3 = knn.predict(X_train)\n    cm1 = confusion_matrix(y_test.iloc[:,label],y_pred[:,label], labels=knn.classes_[label])\n    cm2 = confusion_matrix(y_v.iloc[:,label],y_pred2[:,label], labels=knn.classes_[label])\n    cm3 = confusion_matrix(y_train.iloc[:,label],y_pred3[:,label], labels=knn.classes_[label])\n    disp1 = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=knn.classes_[label])\n    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2,display_labels=knn.classes_[label])\n    disp3 = ConfusionMatrixDisplay(confusion_matrix=cm3,display_labels=knn.classes_[label])\n    print(\"Training accuracy: \", metrics.accuracy_score(y_train.iloc[:,label], y_pred3[:,label]))\n    disp3.plot()\n    plt.title('Old KNN Model (Train)')\n    plt.show()\n    print(\"Validation accuracy: \", metrics.accuracy_score(y_v.iloc[:,label], y_pred2[:,label]))\n    disp2.plot()\n    plt.title('Old KNN Model (Validation)')\n    plt.show()\n    print(\"Test accuracy: \", metrics.accuracy_score(y_test.iloc[:,label], y_pred[:,label]))\n    disp1.plot()\n    plt.title('Old KNN Model (Test)')\n    plt.show()\n    \n    \n    # new best k at k = 78\n    knn = KNeighborsClassifier(n_neighbors=78,weights = 'distance')\n    knn.fit(X_train2, y_train)\n    y_pred = knn.predict(X_test2)\n    y_pred2 = knn.predict(X_v2)\n    y_pred3 = knn.predict(X_train2)\n    cm1 = confusion_matrix(y_test.iloc[:,label],y_pred[:,label], labels=knn.classes_[label])\n    cm2 = confusion_matrix(y_v.iloc[:,label],y_pred2[:,label], labels=knn.classes_[label])\n    cm3 = confusion_matrix(y_train.iloc[:,label],y_pred3[:,label], labels=knn.classes_[label])\n    disp1 = ConfusionMatrixDisplay(confusion_matrix=cm1,display_labels=knn.classes_[label])\n    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm2,display_labels=knn.classes_[label])\n    disp3 = ConfusionMatrixDisplay(confusion_matrix=cm3,display_labels=knn.classes_[label])\n    print(\"Training accuracy: \", metrics.accuracy_score(y_train.iloc[:,label], y_pred3[:,label]))\n    disp3.plot()\n    plt.title('New KNN Model (Train)')\n    plt.show()\n    print(\"Validation accuracy: \", metrics.accuracy_score(y_v.iloc[:,label], y_pred2[:,label]))\n    disp2.plot()\n    plt.title('New KNN Model (Validation)')\n    plt.show()\n    print(\"Test accuracy: \", metrics.accuracy_score(y_test.iloc[:,label], y_pred[:,label]))\n    disp1.plot()\n    plt.title('New KNN Model (Test)')\n    plt.show()\n    \n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that we gain a significant improvement on the validation set after we remove the home state of the speaker. This coincides with our assumption. However, we now see a problem. This dataset could not be generalize at all. To elaborate, assume that a person is a representative from a specific party and tends to lie more, but when he/she becomes the president and he tends to lie less. While the model built from this dataset already pick up a signal from these two feature and may rule him/her out as a liar. Another example would be if once healthcare is a hot debate and there are fake information floating around, would statements about healtcare tend to be fake 2 or 3 years later?\n\nIn conclusion, with yet a tradition NLP and simple classification model, we can achieve accuracy exceeding one from the Yang paper [1]. However, we have some skepticism whether if this dataset ro dataset with similar structure would provide a meaningful method to deliver a generalizable model.","metadata":{}}]}