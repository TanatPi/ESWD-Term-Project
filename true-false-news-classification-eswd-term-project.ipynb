{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Important libaries\nimport pandas as pd\nimport seaborn as sns\nimport nltk\nfrom nltk.tokenize import WhitespaceTokenizer as w_tokenizer\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer=SnowballStemmer(\"english\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-23T09:03:45.923209Z","iopub.execute_input":"2023-09-23T09:03:45.923741Z","iopub.status.idle":"2023-09-23T09:03:45.930267Z","shell.execute_reply.started":"2023-09-23T09:03:45.923702Z","shell.execute_reply":"2023-09-23T09:03:45.929054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_table(\"/kaggle/input/LIARbyYang/train.tsv\", header = None)\ndf.columns = [\"ID\", \"label\", \"statement\", \"subject\", \"speaker\", \"speaker_job\", \"state\", \"party\", \"pof_count\", \"false_count\", \"barelytrue_count\", \"halftrue_count\", \"mostlytrue_count\", \"context\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:45.931667Z","iopub.execute_input":"2023-09-23T09:03:45.932384Z","iopub.status.idle":"2023-09-23T09:03:46.005946Z","shell.execute_reply.started":"2023-09-23T09:03:45.932354Z","shell.execute_reply":"2023-09-23T09:03:46.004722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:46.007818Z","iopub.execute_input":"2023-09-23T09:03:46.008239Z","iopub.status.idle":"2023-09-23T09:03:46.030911Z","shell.execute_reply.started":"2023-09-23T09:03:46.008203Z","shell.execute_reply":"2023-09-23T09:03:46.029884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:46.032534Z","iopub.execute_input":"2023-09-23T09:03:46.032964Z","iopub.status.idle":"2023-09-23T09:03:46.044001Z","shell.execute_reply.started":"2023-09-23T09:03:46.032927Z","shell.execute_reply":"2023-09-23T09:03:46.043213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merge label","metadata":{}},{"cell_type":"code","source":"merger = { 'pants-fire' : 0,\n           'false' : 0,\n           'barely-true': 0,\n           'half-true': 0,\n           'mostly-true': 1,\n           'true': 1}\ndf[\"label\"].replace(merger, inplace=True)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:46.045824Z","iopub.execute_input":"2023-09-23T09:03:46.046852Z","iopub.status.idle":"2023-09-23T09:03:46.077886Z","shell.execute_reply.started":"2023-09-23T09:03:46.046819Z","shell.execute_reply":"2023-09-23T09:03:46.076847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean data","metadata":{}},{"cell_type":"code","source":"# get rid of all periods\ndf['statement'].replace('\\.','',regex=True,inplace=True) \ndf['context'].replace('\\.','',regex=True,inplace=True)\n\n# get rid of all commas\ndf['statement'].replace(',','',regex=True,inplace=True) \ndf['context'].replace(',','',regex=True,inplace=True) \n\n# clean quotations\ndf['statement'].replace('’','\\'',regex=True,inplace=True) \ndf['statement'].replace('‘','\\'',regex=True,inplace=True) \ndf['context'].replace('’','\\'',regex=True,inplace=True) \ndf['context'].replace('‘','\\'',regex=True,inplace=True) \n\ndf['statement'].replace('“','\\\"',regex=True,inplace=True) \ndf['statement'].replace('”','\\\"',regex=True,inplace=True) \ndf['statement'].replace('``','\\\"',regex=True,inplace=True) \ndf['context'].replace('“','\\\"',regex=True,inplace=True) \ndf['context'].replace('”','\\\"',regex=True,inplace=True) \ndf['context'].replace('``','\\\"',regex=True,inplace=True) ","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:46.079010Z","iopub.execute_input":"2023-09-23T09:03:46.079307Z","iopub.status.idle":"2023-09-23T09:03:46.296752Z","shell.execute_reply.started":"2023-09-23T09:03:46.079267Z","shell.execute_reply":"2023-09-23T09:03:46.295833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:46.297980Z","iopub.execute_input":"2023-09-23T09:03:46.298382Z","iopub.status.idle":"2023-09-23T09:03:46.322679Z","shell.execute_reply.started":"2023-09-23T09:03:46.298348Z","shell.execute_reply":"2023-09-23T09:03:46.321615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bag of Words Process on Statements","metadata":{}},{"cell_type":"code","source":"df[\"statement_token\"] = df[\"statement\"].apply(nltk.word_tokenize) # tokenize statement","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:46.323941Z","iopub.execute_input":"2023-09-23T09:03:46.324248Z","iopub.status.idle":"2023-09-23T09:03:47.861328Z","shell.execute_reply.started":"2023-09-23T09:03:46.324222Z","shell.execute_reply":"2023-09-23T09:03:47.860206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['statement_token'].head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:47.862660Z","iopub.execute_input":"2023-09-23T09:03:47.862972Z","iopub.status.idle":"2023-09-23T09:03:47.872302Z","shell.execute_reply.started":"2023-09-23T09:03:47.862945Z","shell.execute_reply":"2023-09-23T09:03:47.871232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['stemmed_statement_token'] = df['statement_token'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:47.876188Z","iopub.execute_input":"2023-09-23T09:03:47.876644Z","iopub.status.idle":"2023-09-23T09:03:50.278467Z","shell.execute_reply.started":"2023-09-23T09:03:47.876614Z","shell.execute_reply":"2023-09-23T09:03:50.277234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vectorize tokens","metadata":{}},{"cell_type":"code","source":"df['stemmed_statement_token']","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:50.280038Z","iopub.execute_input":"2023-09-23T09:03:50.280903Z","iopub.status.idle":"2023-09-23T09:03:50.292294Z","shell.execute_reply.started":"2023-09-23T09:03:50.280861Z","shell.execute_reply":"2023-09-23T09:03:50.291542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See the most common (and potentially useless) words for the dataset","metadata":{}},{"cell_type":"code","source":"# bag of words for the whole dataset\ndef countwords(x):\n    word2count = {}\n    for words in x:\n        for word in words:\n            if word not in word2count.keys():\n                word2count[word] = 1\n            else:\n                word2count[word] += 1\n    # sort the word2count\n    sorted_word2count = sorted(word2count.items(), key=lambda x:x[1], reverse = True)\n    return sorted_word2count\n            \nwords_in_statements = countwords(df[\"stemmed_statement_token\"])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:50.293461Z","iopub.execute_input":"2023-09-23T09:03:50.293922Z","iopub.status.idle":"2023-09-23T09:03:50.370889Z","shell.execute_reply.started":"2023-09-23T09:03:50.293895Z","shell.execute_reply":"2023-09-23T09:03:50.369789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check for redundant words:","metadata":{}},{"cell_type":"code","source":"words_in_statements[0:20]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:50.372625Z","iopub.execute_input":"2023-09-23T09:03:50.373241Z","iopub.status.idle":"2023-09-23T09:03:50.379228Z","shell.execute_reply.started":"2023-09-23T09:03:50.373208Z","shell.execute_reply":"2023-09-23T09:03:50.378512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply CountVectorizer, ignore the first few rendundant words and words that appear only once.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer=lambda x: x, max_df = 1400 ,min_df = 2) # need analyzer=lambda x: x to disable the analyzer, since the input is pandas series of lists.\nX = vectorizer.fit_transform(df['stemmed_statement_token'])\nnew_features = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names_out()) # Get new features from countvectorizer\nnew_features.columns = 'statement_' + new_features.columns # add prefix to the column names\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:50.380839Z","iopub.execute_input":"2023-09-23T09:03:50.381510Z","iopub.status.idle":"2023-09-23T09:03:50.696821Z","shell.execute_reply.started":"2023-09-23T09:03:50.381462Z","shell.execute_reply":"2023-09-23T09:03:50.695951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Merge new features","metadata":{}},{"cell_type":"code","source":"df2 = pd.concat([df,new_features],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:50.698232Z","iopub.execute_input":"2023-09-23T09:03:50.698915Z","iopub.status.idle":"2023-09-23T09:03:51.690645Z","shell.execute_reply.started":"2023-09-23T09:03:50.698877Z","shell.execute_reply":"2023-09-23T09:03:51.689775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do the same for context. But there are 102 null present, drop them first.","metadata":{}},{"cell_type":"code","source":"sum(df2['context'].isnull())","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:51.691887Z","iopub.execute_input":"2023-09-23T09:03:51.692208Z","iopub.status.idle":"2023-09-23T09:03:51.701313Z","shell.execute_reply.started":"2023-09-23T09:03:51.692181Z","shell.execute_reply":"2023-09-23T09:03:51.700328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop rows with null context\ndf2.dropna(subset=['context'], inplace=True, ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:51.702570Z","iopub.execute_input":"2023-09-23T09:03:51.702894Z","iopub.status.idle":"2023-09-23T09:03:51.937121Z","shell.execute_reply.started":"2023-09-23T09:03:51.702867Z","shell.execute_reply":"2023-09-23T09:03:51.936010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2[\"context_token\"] = df2[\"context\"].apply(nltk.word_tokenize) # tokenize statement\ndf2['stemmed_context_token'] = df2['context_token'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:51.938553Z","iopub.execute_input":"2023-09-23T09:03:51.938957Z","iopub.status.idle":"2023-09-23T09:03:53.331398Z","shell.execute_reply.started":"2023-09-23T09:03:51.938921Z","shell.execute_reply":"2023-09-23T09:03:53.330569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:53.332613Z","iopub.execute_input":"2023-09-23T09:03:53.333684Z","iopub.status.idle":"2023-09-23T09:03:53.341128Z","shell.execute_reply.started":"2023-09-23T09:03:53.333651Z","shell.execute_reply":"2023-09-23T09:03:53.339935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check redundant:","metadata":{}},{"cell_type":"code","source":"words_in_context = countwords(df2[\"stemmed_context_token\"])\nprint(words_in_context[0:20])","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:53.342851Z","iopub.execute_input":"2023-09-23T09:03:53.343336Z","iopub.status.idle":"2023-09-23T09:03:53.368746Z","shell.execute_reply.started":"2023-09-23T09:03:53.343273Z","shell.execute_reply":"2023-09-23T09:03:53.367366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(words_in_context)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:53.370388Z","iopub.execute_input":"2023-09-23T09:03:53.371172Z","iopub.status.idle":"2023-09-23T09:03:53.384935Z","shell.execute_reply.started":"2023-09-23T09:03:53.371142Z","shell.execute_reply":"2023-09-23T09:03:53.384156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply vectorizer:","metadata":{}},{"cell_type":"code","source":"cv = CountVectorizer(analyzer=lambda x: x, max_df = 1800 ,min_df = 4) # need analyzer=lambda x: x to disable the analyzer, since the input is pandas series of lists.\nX2 = cv.fit_transform(df2['stemmed_context_token'])\nnew_features_context = pd.DataFrame(X2.toarray(), columns = cv.get_feature_names_out()) # Get new features from countvectorizer\nnew_features_context.columns = 'context_' + new_features_context.columns # add prefix to the column names\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:53.386221Z","iopub.execute_input":"2023-09-23T09:03:53.386545Z","iopub.status.idle":"2023-09-23T09:03:53.468806Z","shell.execute_reply.started":"2023-09-23T09:03:53.386519Z","shell.execute_reply":"2023-09-23T09:03:53.467786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_features_context","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:53.473406Z","iopub.execute_input":"2023-09-23T09:03:53.473760Z","iopub.status.idle":"2023-09-23T09:03:53.499344Z","shell.execute_reply.started":"2023-09-23T09:03:53.473729Z","shell.execute_reply":"2023-09-23T09:03:53.498298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3 = pd.concat([df2,new_features_context],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:53.500367Z","iopub.execute_input":"2023-09-23T09:03:53.500658Z","iopub.status.idle":"2023-09-23T09:03:53.769963Z","shell.execute_reply.started":"2023-09-23T09:03:53.500626Z","shell.execute_reply":"2023-09-23T09:03:53.768909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop original columns:","metadata":{}},{"cell_type":"code","source":"df3.drop(columns=['statement', 'statement_token', 'stemmed_statement_token','context', 'context_token', 'stemmed_context_token'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:53.771318Z","iopub.execute_input":"2023-09-23T09:03:53.774602Z","iopub.status.idle":"2023-09-23T09:03:54.011238Z","shell.execute_reply.started":"2023-09-23T09:03:53.774556Z","shell.execute_reply":"2023-09-23T09:03:54.010082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do the same for subject. Now, the tokens are separated by commas in this case. Thus, change them to spaces before tokenizing them.","metadata":{}},{"cell_type":"code","source":"df3['subject'].replace(',',' ',regex=True,inplace=True)  # turn commas to blank spaces\ndf3[\"subject_token\"] = df3[\"subject\"].apply(nltk.word_tokenize) # tokenize statement\ndf3['stemmed_subject_token'] = df3['subject_token'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:54.013005Z","iopub.execute_input":"2023-09-23T09:03:54.013601Z","iopub.status.idle":"2023-09-23T09:03:55.381717Z","shell.execute_reply.started":"2023-09-23T09:03:54.013558Z","shell.execute_reply":"2023-09-23T09:03:55.380489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check redundant:","metadata":{}},{"cell_type":"code","source":"words_in_subject = countwords(df3[\"stemmed_subject_token\"])\nprint(words_in_subject[0:20])","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:55.383009Z","iopub.execute_input":"2023-09-23T09:03:55.383375Z","iopub.status.idle":"2023-09-23T09:03:55.398675Z","shell.execute_reply.started":"2023-09-23T09:03:55.383346Z","shell.execute_reply":"2023-09-23T09:03:55.397578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply CountVectorizer:","metadata":{}},{"cell_type":"code","source":"cvv = CountVectorizer(analyzer=lambda x: x, max_df = 1.0 ,min_df = 1) # need analyzer=lambda x: x to disable the analyzer, since the input is pandas series of lists.\nX3 = cvv.fit_transform(df3['stemmed_subject_token'])\nnew_features_subject = pd.DataFrame(X3.toarray(), columns = cvv.get_feature_names_out()) # Get new features from countvectorizer\nnew_features_subject.columns = 'subject_' + new_features_subject.columns # add prefix to the column names\n","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:55.400296Z","iopub.execute_input":"2023-09-23T09:03:55.401048Z","iopub.status.idle":"2023-09-23T09:03:55.450045Z","shell.execute_reply.started":"2023-09-23T09:03:55.401007Z","shell.execute_reply":"2023-09-23T09:03:55.448860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_features_subject","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:55.451449Z","iopub.execute_input":"2023-09-23T09:03:55.452079Z","iopub.status.idle":"2023-09-23T09:03:55.477576Z","shell.execute_reply.started":"2023-09-23T09:03:55.452031Z","shell.execute_reply":"2023-09-23T09:03:55.476361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the final DataFrame:","metadata":{}},{"cell_type":"code","source":"df4 = pd.concat([df3,new_features_subject],axis=1)\ndf4.drop(columns=['subject', 'subject_token', 'stemmed_subject_token'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:07:24.137922Z","iopub.execute_input":"2023-09-23T09:07:24.138468Z","iopub.status.idle":"2023-09-23T09:07:24.500639Z","shell.execute_reply.started":"2023-09-23T09:07:24.138425Z","shell.execute_reply":"2023-09-23T09:07:24.499722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df4.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:07:26.068342Z","iopub.execute_input":"2023-09-23T09:07:26.069131Z","iopub.status.idle":"2023-09-23T09:07:26.075811Z","shell.execute_reply.started":"2023-09-23T09:07:26.069087Z","shell.execute_reply":"2023-09-23T09:07:26.074958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"df.subject.unique().shape","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:55.704649Z","iopub.execute_input":"2023-09-23T09:03:55.705484Z","iopub.status.idle":"2023-09-23T09:03:55.717824Z","shell.execute_reply.started":"2023-09-23T09:03:55.705450Z","shell.execute_reply":"2023-09-23T09:03:55.716787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(data=df3, x='subject', kind=\"count\", palette=\"ch:.25\",hue=\"label\")","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:03:55.718906Z","iopub.execute_input":"2023-09-23T09:03:55.719189Z","iopub.status.idle":"2023-09-23T09:05:17.893861Z","shell.execute_reply.started":"2023-09-23T09:03:55.719165Z","shell.execute_reply":"2023-09-23T09:05:17.892524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from copy import deepcopy\ndf_test = deepcopy(df)\ndf_test['subject'] = df['subject'].mask(df.groupby('subject')['subject'].transform('size').lt(60), 'Others')\ndf_test.drop(df_test[df_test['subject'] ==  'Others'].index, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:05:17.895151Z","iopub.execute_input":"2023-09-23T09:05:17.895476Z","iopub.status.idle":"2023-09-23T09:05:17.922197Z","shell.execute_reply.started":"2023-09-23T09:05:17.895449Z","shell.execute_reply":"2023-09-23T09:05:17.920966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['subject'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:05:17.923738Z","iopub.execute_input":"2023-09-23T09:05:17.924062Z","iopub.status.idle":"2023-09-23T09:05:17.933927Z","shell.execute_reply.started":"2023-09-23T09:05:17.924036Z","shell.execute_reply":"2023-09-23T09:05:17.932778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(data=df_test, x='subject', kind=\"count\", palette=\"ch:.25\",hue=\"label\")","metadata":{"execution":{"iopub.status.busy":"2023-09-23T09:05:17.935531Z","iopub.execute_input":"2023-09-23T09:05:17.935891Z","iopub.status.idle":"2023-09-23T09:05:18.532863Z","shell.execute_reply.started":"2023-09-23T09:05:17.935853Z","shell.execute_reply":"2023-09-23T09:05:18.531581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show unique value for \"context\"","metadata":{}},{"cell_type":"markdown","source":"Show all rows that contain:","metadata":{}}]}